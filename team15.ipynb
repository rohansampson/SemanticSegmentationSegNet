{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 for Cardiac MR Image Segmentation (2020-2021)\n",
    "\n",
    "The aim of this work is to create a neural network that segments cardiac MR images. The dataset is comprised of 200 96x96 greyscale cardiac MR images (CMRI) along the short axis. The goal is to segment them into four different categories: myocardium, left ventricle, right ventricle, and background. The data has been pulled from the Automated Caridiac Diagnosis Challenge (ACDC) (O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, 2018). Below is an example of a CMRI (left) and the segmentation (called mask) (right).\n",
    "![alt text](sample_dataset.png \"Example of image and true segmentation\")\n",
    "\n",
    "CMRI allows for a non-invasive examination of the heart. Appliying image segmentation allows for calculation of important quantitative measures such as right and left ventricle volume, myocardial mass, and ejection fraction (Chen, Chen et al., 2020).\n",
    "\n",
    "This is the ideal problem for a convolutional neural network - indeed as technology and deep learning have developed, the area of medical imagery has benefited greatly. Applying a CNN to pixel-wise classification allows for a much faster and less error-prone method of segmentation.\n",
    "\n",
    "The dataset has been split into the following three subsets: 50% for training, 10% for validation, and the remaining 40% for testing. The goal of this work is to implement a known segmentation architecture to this dataset then perform hyper-parameter optimisation to maximise the dice score on the test set. The dice score between two generic masks A, B is defined as:\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?dice(A,&space;B)&space;=&space;\\frac{2|A&space;\\cap&space;B|}{|A|&plus;|B|}\" title=\"dice(A, B) = \\frac{2|A \\cap B|}{|A|+|B|}\" />\n",
    "The dice score ranges between 0 and 1 with 0 being completely wrong and 1 being a perfect match. Since there are 4 classes to be segmented then even if the theoretical minimum is 0, by just classiying each pixel at random will achieve a dice score of 0.25. \n",
    "\n",
    "Below we define some two plotting functions: one that plots the image with the ground truth mask and another that plots the image with the ground truth mask and the predicted mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def show_image_mask(img, mask, cmap='gray'): # visualisation\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    \n",
    "#Additional function that plots the predicted segmentation along with the image and the ground truth\n",
    "def show_image_mask_pred(img, mask, pred, cmap='gray'):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img, cmap=cmap) #CMRI\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap=cmap) #ground truth mask\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred, cmap=cmap) # predicted mask\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two classes of data loaders to store the training, validating, and test data in memory. Note that the validation data loader will be an instance of the `TrainDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            basename = os.path.basename(img_path)\n",
    "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            basename = os.path.basename(img_path)\n",
    "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "Now we define the network architecture. We opted to use the SegNet architecture (Badrinarayanan, Kendall and Cipolla, 2015) which is an encoder-decoder model for image segmentation. The idea is that the model applies a sequence of increasingly large number of convolutional filters and max-pooling layers to map the image to a low dimensional space (but with a large number of channels) and then to reverse the process by up-sampling and convolutional layers to return the image back to the same dimensionality of the input. Below is the architecture of SegNet taken from their paper.\n",
    "\n",
    "![alt text](segnet_archi.png \"Architecture of SegNet\")\n",
    "\n",
    "The first half of the network is the encoder and is topologically identical to the first 13 convolutional layers of the VGG16 architecture. Indeed, in the paper the authors apply transfer learning and initialise the encoder weights with the VGG16 weights. Then after the encoder there are 13 decoder layers each one coresponding to an encoder layer. At the end, instead of having a fully connected layer (such as the one found in VGG16), SegNet opts to map straight from the high-resolution feature map straight to a pixel-wise softmax classification. Each convolutional layer uses a 3x3 kernel with 1 padding. The number of filters can be seen in the code below. The other notable feature of SegNet is the connection between each max-pooling layer and its corresponding up-sampling. When the down-pooling occurs, the model stores the index of where the maximum value in the max-pooling occurs and uses those indices to up-sample. Below we define the model in `CNNSEG()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNSEG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNSEG, self).__init__()\n",
    "        # fill in the constructor for your model here\n",
    "        #Encoder layers\n",
    "        self.encoder_00 = nn.Sequential(*[nn.Conv2d(in_channels=1,out_channels=64,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(64)])\n",
    "        \n",
    "        self.encoder_01 = nn.Sequential(*[nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(64)])\n",
    "        \n",
    "        self.encoder_10 = nn.Sequential(*[nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(128)])\n",
    "        \n",
    "        self.encoder_11 = nn.Sequential(*[nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(128)])\n",
    "        \n",
    "        self.encoder_20 = nn.Sequential(*[nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.encoder_21 = nn.Sequential(*[nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.encoder_22 = nn.Sequential(*[nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.encoder_30 = nn.Sequential(*[nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_31 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_32 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_40 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_41 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_42 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        #Decoder layers\n",
    "        \n",
    "        \n",
    "        self.decoder_42 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_41 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_40 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_32 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_31 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_30 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.decoder_22 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=256,out_channels=256,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.decoder_21 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=256,out_channels=256,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.decoder_20 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(128)])\n",
    "        \n",
    "        self.decoder_11 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(128)])\n",
    "        \n",
    "        self.decoder_10 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(64)])\n",
    "        \n",
    "        self.decoder_01 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(64)])\n",
    "        \n",
    "        self.decoder_00 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=64,out_channels=4,kernel_size=3,padding=1)])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "\n",
    "        # Encoder Stage - 1\n",
    "        dim_0 = x.size()\n",
    "        x_00 = F.relu(self.encoder_00(x))\n",
    "        x_01 = F.relu(self.encoder_01(x_00))\n",
    "        x_0, indices_0 = F.max_pool2d(x_01, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Encoder Stage - 2\n",
    "        dim_1 = x_0.size()\n",
    "        x_10 = F.relu(self.encoder_10(x_0))\n",
    "        x_11 = F.relu(self.encoder_11(x_10))\n",
    "        x_1, indices_1 = F.max_pool2d(x_11, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Encoder Stage - 3\n",
    "        dim_2 = x_1.size()\n",
    "        x_20 = F.relu(self.encoder_20(x_1))\n",
    "        x_21 = F.relu(self.encoder_21(x_20))\n",
    "        x_22 = F.relu(self.encoder_22(x_21))\n",
    "        x_2, indices_2 = F.max_pool2d(x_22, kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Encoder Stage - 4\n",
    "        dim_3 = x_2.size()\n",
    "        x_30 = F.relu(self.encoder_30(x_2))\n",
    "        x_31 = F.relu(self.encoder_31(x_30))\n",
    "        x_32 = F.relu(self.encoder_32(x_31))\n",
    "        x_3, indices_3 = F.max_pool2d(x_32, kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Encoder Stage - 5\n",
    "        dim_4 = x_3.size()\n",
    "        x_40 = F.relu(self.encoder_40(x_3))\n",
    "        x_41 = F.relu(self.encoder_41(x_40))\n",
    "        x_42 = F.relu(self.encoder_42(x_41))\n",
    "        x_4, indices_4 = F.max_pool2d(x_42, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Decoder\n",
    "\n",
    "        dim_d = x_4.size()\n",
    "\n",
    "        # Decoder Stage - 5\n",
    "        x_4d = F.max_unpool2d(x_4, indices_4, kernel_size=2, stride=2, output_size=dim_4)\n",
    "        x_42d = F.relu(self.decoder_42(x_4d))\n",
    "        x_41d = F.relu(self.decoder_41(x_42d))\n",
    "        x_40d = F.relu(self.decoder_40(x_41d))\n",
    "        \n",
    "        # Decoder Stage - 4\n",
    "        x_3d = F.max_unpool2d(x_3, indices_3, kernel_size=2, stride=2, output_size=dim_3)\n",
    "        x_32d = F.relu(self.decoder_32(x_3d))\n",
    "        x_31d = F.relu(self.decoder_31(x_32d))\n",
    "        x_30d = F.relu(self.decoder_30(x_31d))\n",
    "        \n",
    "        # Decoder Stage - 3\n",
    "        x_2d = F.max_unpool2d(x_30d, indices_2, kernel_size=2, stride=2, output_size=dim_2)\n",
    "        x_22d = F.relu(self.decoder_22(x_2d))\n",
    "        x_21d = F.relu(self.decoder_21(x_22d))\n",
    "        x_20d = F.relu(self.decoder_20(x_21d))\n",
    "\n",
    "        # Decoder Stage - 2\n",
    "        x_1d = F.max_unpool2d(x_20d, indices_1, kernel_size=2, stride=2, output_size=dim_1)\n",
    "        x_11d = F.relu(self.decoder_11(x_1d))\n",
    "        x_10d = F.relu(self.decoder_10(x_11d))\n",
    "\n",
    "        # Decoder Stage - 1\n",
    "        x_0d = F.max_unpool2d(x_10d, indices_0, kernel_size=2, stride=2, output_size=dim_0)\n",
    "        x_01d = F.relu(self.decoder_01(x_0d))\n",
    "        x_00d = self.decoder_00(x_01d)\n",
    "\n",
    "        x_softmax = F.softmax(x_00d, dim=1)\n",
    "\n",
    "        return x_softmax\n",
    "\n",
    "model = CNNSEG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the summary of the model including the shape of the tensor at each step and the number of trainable parameters. Due to the fact that the network is fully convolutional and the use of max-pooling indices in up-sampling allows the model to have relatively less trainable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 96, 96]             640\n",
      "       BatchNorm2d-2           [-1, 64, 96, 96]             128\n",
      "            Conv2d-3           [-1, 64, 96, 96]          36,928\n",
      "       BatchNorm2d-4           [-1, 64, 96, 96]             128\n",
      "            Conv2d-5          [-1, 128, 48, 48]          73,856\n",
      "       BatchNorm2d-6          [-1, 128, 48, 48]             256\n",
      "            Conv2d-7          [-1, 128, 48, 48]         147,584\n",
      "       BatchNorm2d-8          [-1, 128, 48, 48]             256\n",
      "            Conv2d-9          [-1, 256, 24, 24]         295,168\n",
      "      BatchNorm2d-10          [-1, 256, 24, 24]             512\n",
      "           Conv2d-11          [-1, 256, 24, 24]         590,080\n",
      "      BatchNorm2d-12          [-1, 256, 24, 24]             512\n",
      "           Conv2d-13          [-1, 256, 24, 24]         590,080\n",
      "      BatchNorm2d-14          [-1, 256, 24, 24]             512\n",
      "           Conv2d-15          [-1, 512, 12, 12]       1,180,160\n",
      "      BatchNorm2d-16          [-1, 512, 12, 12]           1,024\n",
      "           Conv2d-17          [-1, 512, 12, 12]       2,359,808\n",
      "      BatchNorm2d-18          [-1, 512, 12, 12]           1,024\n",
      "           Conv2d-19          [-1, 512, 12, 12]       2,359,808\n",
      "      BatchNorm2d-20          [-1, 512, 12, 12]           1,024\n",
      "           Conv2d-21            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-22            [-1, 512, 6, 6]           1,024\n",
      "           Conv2d-23            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-24            [-1, 512, 6, 6]           1,024\n",
      "           Conv2d-25            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-26            [-1, 512, 6, 6]           1,024\n",
      "  ConvTranspose2d-27            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-28            [-1, 512, 6, 6]           1,024\n",
      "  ConvTranspose2d-29            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-30            [-1, 512, 6, 6]           1,024\n",
      "  ConvTranspose2d-31            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-32            [-1, 512, 6, 6]           1,024\n",
      "  ConvTranspose2d-33          [-1, 512, 12, 12]       2,359,808\n",
      "      BatchNorm2d-34          [-1, 512, 12, 12]           1,024\n",
      "  ConvTranspose2d-35          [-1, 512, 12, 12]       2,359,808\n",
      "      BatchNorm2d-36          [-1, 512, 12, 12]           1,024\n",
      "  ConvTranspose2d-37          [-1, 256, 12, 12]       1,179,904\n",
      "      BatchNorm2d-38          [-1, 256, 12, 12]             512\n",
      "  ConvTranspose2d-39          [-1, 256, 24, 24]         590,080\n",
      "      BatchNorm2d-40          [-1, 256, 24, 24]             512\n",
      "  ConvTranspose2d-41          [-1, 256, 24, 24]         590,080\n",
      "      BatchNorm2d-42          [-1, 256, 24, 24]             512\n",
      "  ConvTranspose2d-43          [-1, 128, 24, 24]         295,040\n",
      "      BatchNorm2d-44          [-1, 128, 24, 24]             256\n",
      "  ConvTranspose2d-45          [-1, 128, 48, 48]         147,584\n",
      "      BatchNorm2d-46          [-1, 128, 48, 48]             256\n",
      "  ConvTranspose2d-47           [-1, 64, 48, 48]          73,792\n",
      "      BatchNorm2d-48           [-1, 64, 48, 48]             128\n",
      "  ConvTranspose2d-49           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-50           [-1, 64, 96, 96]             128\n",
      "  ConvTranspose2d-51            [-1, 4, 96, 96]           2,308\n",
      "================================================================\n",
      "Total params: 29,444,164\n",
      "Trainable params: 29,444,164\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 63.28\n",
      "Params size (MB): 112.32\n",
      "Estimated Total Size (MB): 175.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (1, 96, 96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordance with the SegNet paper, the loss function was chosen to be categorical cross-entropy. This is the natural choice for classification problems and is perfect for our case as we are performing pixel-wise classification. Unlike in the paper, we opted to use the Adam optimiser as it is currently one of the most popular optimisers. We allow pytorch to use GPU acceleration if it is available. Lastly, we initialise the learning rate at 0.001 as well as training and validation loss lists to store the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the learning rate\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "#Initialise cross entropy loss and adam optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=LEARNING_RATE)\n",
    "\n",
    "#if gpu is available then it sends it to gpu else it uses cpu \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "#Initialise list to store loss over training\n",
    "train_loss = list()\n",
    "val_loss = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define some helper functions:\n",
    "- `categorical_dice` which computes the categorical dice between two masks for a specific class.\n",
    "- `forward` which preprocesses the image tensor by making it the right dimension and dividing each element component-wise by 255 to make the tensor lie inside the hypercube (granted of very high dimension). The gradients get set to zero in the optimiser and we push the mini-batch forwards through the network and return the softmax output of the model as a tensor of shape Bx4x96x96 where B is the batch size, 4 corresponds to the four categories and 96x96 is the image pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical dice score between mask1 and mask2 along label_class\n",
    "def categorical_dice(mask1, mask2, label_class=1):\n",
    "    mask1_pos = (mask1 == label_class).numpy()\n",
    "    mask2_pos = (mask2 == label_class).numpy()\n",
    "    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
    "    return dice\n",
    "\n",
    "#Packages proprocessing and forward pass\n",
    "def forward(img):\n",
    "    #scales image so that each pixel lies between 0 and 1\n",
    "    img_scale = img/255.\n",
    "    \n",
    "    #reshapes the tensor \n",
    "    img_scale = img_scale.unsqueeze(1)\n",
    "    \n",
    "    #resets optimiser \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #pushes processed image through network\n",
    "    seg_soft = model(img_scale)\n",
    "    \n",
    "    return seg_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_path = './data/train'\n",
    "val_path = './data/val'\n",
    "num_workers = 8\n",
    "batch_size = 10\n",
    "train_set = TrainDataset(data_path)\n",
    "val_set = TrainDataset(val_path)\n",
    "\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "validating_data_loader = DataLoader(dataset=val_set, num_workers=num_workers, batch_size=20, shuffle=True)\n",
    "\n",
    "\n",
    "data_loader = {'train':training_data_loader,\n",
    "              'validate':validating_data_loader}\n",
    "\n",
    "EPOCHS = 0\n",
    "Max_Epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5 #Tolerance perameter\n",
    "print('Loss function ', loss_fn )\n",
    "print('optimizer ', optimizer)\n",
    "print('Mini Batch size set to ', batch_size)\n",
    "print(device)\n",
    "\n",
    "# Fetch images and labels. \n",
    "exit = False\n",
    "while not exit:\n",
    "    print('-' * 30)\n",
    "    print('EPOCHS = ',EPOCHS)\n",
    "\n",
    "    model.train(True)\n",
    "    batch_loss = 0.0\n",
    "\n",
    "    for index_train, sample in enumerate(data_loader['train']):\n",
    "        \n",
    "        img, mask = sample\n",
    "        \n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        \n",
    "        soft_mask = forward(img)\n",
    "        \n",
    "        loss = loss_fn(soft_mask, mask.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "    train_loss.append(batch_loss / (index_train + 1))\n",
    "    print(\"Train loss: \", train_loss[-1])\n",
    "    \n",
    "    model.train(False)\n",
    "\n",
    "    batch_loss = 0.0\n",
    "    for index_val, sample in enumerate(data_loader['validate']):\n",
    "        img, mask = sample\n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        soft_mask = forward(img)\n",
    "       \n",
    "        loss = loss_fn(soft_mask, mask.long())\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "    val_loss.append(batch_loss / (index_val + 1))\n",
    "    print(\"Validation loss: \", val_loss[-1])\n",
    "    \n",
    "    if EPOCHS > 2:\n",
    "        if (np.absolute(val_loss[-1] - val_loss[-2]) < tol) or (EPOCHS == Max_Epoch ):\n",
    "            exit = True\n",
    "    EPOCHS += 1\n",
    "    \n",
    "if (EPOCHS-1) == Max_Epoch:\n",
    "    print('Failed to converge')\n",
    "else:\n",
    "    print('Converged in ',EPOCHS-1, ' epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# References\n",
    "Badrinarayanan, V., Kendall, A. and Cipolla, R., 2015. Segnet: A Deep Convolutional Encoder-Decoder Architecture For Image Segmentation. [online] arXiv.org. Available at: <https://arxiv.org/abs/1511.00561v3> [Accessed 14 December 2020].\n",
    "\n",
    "O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, et al.\n",
    "\"Deep Learning Techniques for Automatic MRI Cardiac Multi-structures Segmentation and\n",
    "Diagnosis: Is the Problem Solved ?\" in IEEE Transactions on Medical Imaging,\n",
    "vol. 37, no. 11, pp. 2514-2525, Nov. 2018\n",
    "doi: 10.1109/TMI.2018.2837502\n",
    "\n",
    "Chen, Chen et al. “Deep Learning for Cardiac Image Segmentation: A Review.” Frontiers in cardiovascular medicine vol. 7 25. 5 Mar. 2020, doi:10.3389/fcvm.2020.00025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
