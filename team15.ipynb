{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 for Cardiac MR Image Segmentation (2020-2021)\n",
    "\n",
    "The aim of this work is to create a neural network that segments cardiac MR images. The dataset is comprised of 200 96x96 greyscale cardiac MR images (CMRI) along the short axis. The goal is to segment them into four different categories: myocardium, left ventricle, right ventricle, and background. The data has been pulled from the Automated Caridiac Diagnosis Challenge (ACDC) (O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, 2018). Below is an example of a CMRI (left) and the segmentation (called mask) (right).\n",
    "![alt text](sample_dataset.png \"Example of image and true segmentation\")\n",
    "\n",
    "CMRI allows for a non-invasive examination of the heart. Appliying image segmentation allows for calculation of important quantitative measures such as right and left ventricle volume, myocardial mass, and ejection fraction (Chen, Chen et al., 2020).\n",
    "\n",
    "This is the ideal problem for a convolutional neural network - indeed as technology and deep learning have developed, the area of medical imagery has benefited greatly. Applying a CNN to pixel-wise classification allows for a much faster and less error-prone method of segmentation.\n",
    "\n",
    "The dataset has been split into the following three subsets: 50% for training, 10% for validation, and the remaining 40% for testing. The goal of this work is to implement a known segmentation architecture to this dataset then perform hyper-parameter optimisation to maximise the dice score on the test set. The dice score between two generic masks A, B is defined as:\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?dice(A,&space;B)&space;=&space;\\frac{2|A&space;\\cap&space;B|}{|A|&plus;|B|}\" title=\"dice(A, B) = \\frac{2|A \\cap B|}{|A|+|B|}\" />\n",
    "The dice score ranges between 0 and 1 with 0 being completely wrong and 1 being a perfect match. Since there are 4 classes to be segmented then even if the theoretical minimum is 0, by just classiying each pixel at random will achieve a dice score of 0.25 (that is if each class occurs with equal frequency).\n",
    "\n",
    "Below we define some two plotting functions: one that plots the image with the ground truth mask and another that plots the image with the ground truth mask and the predicted mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def show_image_mask(img, mask, cmap='gray'): # visualisation\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    \n",
    "#Additional function that plots the predicted segmentation along with the image and the ground truth\n",
    "def show_image_mask_pred(img, mask, pred, cmap='gray'):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img, cmap=cmap) #CMRI\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap=cmap) #ground truth mask\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred, cmap=cmap) # predicted mask\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two classes of data loaders to store the training, validating, and test data in memory. Note that the validation data loader will be an instance of the `TrainDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            basename = os.path.basename(img_path)\n",
    "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            basename = os.path.basename(img_path)\n",
    "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "Now we define the network architecture. We opted to use the SegNet architecture (Badrinarayanan, Kendall and Cipolla, 2015) which is an encoder-decoder model for image segmentation. The idea is that the model applies a sequence of increasingly large number of convolutional filters and max-pooling layers to map the image to a low dimensional space (but with a large number of channels) and then to reverse the process by up-sampling and convolutional layers to return the image back to the same dimensionality of the input. Below is the architecture of SegNet taken from their paper.\n",
    "\n",
    "![alt text](segnet_archi.png \"Architecture of SegNet\")\n",
    "\n",
    "The first half of the network is the encoder and is topologically identical to the first 13 convolutional layers of the VGG16 architecture. Indeed, in the paper the authors apply transfer learning and initialise the encoder weights with the VGG16 weights. Then after the encoder there are 13 decoder layers each one coresponding to an encoder layer. At the end, instead of having a fully connected layer (such as the one found in VGG16), SegNet opts to map straight from the high-resolution feature map straight to a pixel-wise softmax classification. Each convolutional layer uses a 3x3 kernel with 1 padding. The number of filters can be seen in the code below. The other notable feature of SegNet is the connection between each max-pooling layer and its corresponding up-sampling. When the down-pooling occurs, the model stores the index of where the maximum value in the max-pooling occurs and uses those indices to up-sample. Below we define the model in `CNNSEG()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNSEG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNSEG, self).__init__()\n",
    "        # fill in the constructor for your model here\n",
    "        #Encoder layers\n",
    "        self.encoder_00 = nn.Sequential(*[nn.Conv2d(in_channels=1,out_channels=64,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(64)])\n",
    "        \n",
    "        self.encoder_01 = nn.Sequential(*[nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(64)])\n",
    "        \n",
    "        self.encoder_10 = nn.Sequential(*[nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(128)])\n",
    "        \n",
    "        self.encoder_11 = nn.Sequential(*[nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(128)])\n",
    "        \n",
    "        self.encoder_20 = nn.Sequential(*[nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.encoder_21 = nn.Sequential(*[nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.encoder_22 = nn.Sequential(*[nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.encoder_30 = nn.Sequential(*[nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_31 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_32 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_40 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_41 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.encoder_42 = nn.Sequential(*[nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        #Decoder layers\n",
    "        \n",
    "        \n",
    "        self.decoder_42 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_41 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_40 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_32 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_31 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=512,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(512)])\n",
    "        \n",
    "        self.decoder_30 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512,out_channels=256,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.decoder_22 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=256,out_channels=256,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.decoder_21 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=256,out_channels=256,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(256)])\n",
    "        \n",
    "        self.decoder_20 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=256,out_channels=128,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(128)])\n",
    "        \n",
    "        self.decoder_11 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=128,out_channels=128,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(128)])\n",
    "        \n",
    "        self.decoder_10 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=128,out_channels=64,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(64)])\n",
    "        \n",
    "        self.decoder_01 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=64,out_channels=64,\n",
    "                                                             kernel_size=3,padding=1),\n",
    "                                                nn.BatchNorm2d(64)])\n",
    "        \n",
    "        self.decoder_00 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=64,out_channels=4,\n",
    "                                                             kernel_size=3,padding=1)])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "\n",
    "        # Encoder Stage - 1\n",
    "        dim_0 = x.size()\n",
    "        x_00 = F.relu(self.encoder_00(x))\n",
    "        x_01 = F.relu(self.encoder_01(x_00))\n",
    "        x_0, indices_0 = F.max_pool2d(x_01, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Encoder Stage - 2\n",
    "        dim_1 = x_0.size()\n",
    "        x_10 = F.relu(self.encoder_10(x_0))\n",
    "        x_11 = F.relu(self.encoder_11(x_10))\n",
    "        x_1, indices_1 = F.max_pool2d(x_11, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Encoder Stage - 3\n",
    "        dim_2 = x_1.size()\n",
    "        x_20 = F.relu(self.encoder_20(x_1))\n",
    "        x_21 = F.relu(self.encoder_21(x_20))\n",
    "        x_22 = F.relu(self.encoder_22(x_21))\n",
    "        x_2, indices_2 = F.max_pool2d(x_22, kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Encoder Stage - 4\n",
    "        dim_3 = x_2.size()\n",
    "        x_30 = F.relu(self.encoder_30(x_2))\n",
    "        x_31 = F.relu(self.encoder_31(x_30))\n",
    "        x_32 = F.relu(self.encoder_32(x_31))\n",
    "        x_3, indices_3 = F.max_pool2d(x_32, kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Encoder Stage - 5\n",
    "        dim_4 = x_3.size()\n",
    "        x_40 = F.relu(self.encoder_40(x_3))\n",
    "        x_41 = F.relu(self.encoder_41(x_40))\n",
    "        x_42 = F.relu(self.encoder_42(x_41))\n",
    "        x_4, indices_4 = F.max_pool2d(x_42, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Decoder\n",
    "\n",
    "        dim_d = x_4.size()\n",
    "\n",
    "        # Decoder Stage - 5\n",
    "        x_4d = F.max_unpool2d(x_4, indices_4, kernel_size=2, stride=2, output_size=dim_4)\n",
    "        x_42d = F.relu(self.decoder_42(x_4d))\n",
    "        x_41d = F.relu(self.decoder_41(x_42d))\n",
    "        x_40d = F.relu(self.decoder_40(x_41d))\n",
    "        \n",
    "        # Decoder Stage - 4\n",
    "        x_3d = F.max_unpool2d(x_3, indices_3, kernel_size=2, stride=2, output_size=dim_3)\n",
    "        x_32d = F.relu(self.decoder_32(x_3d))\n",
    "        x_31d = F.relu(self.decoder_31(x_32d))\n",
    "        x_30d = F.relu(self.decoder_30(x_31d))\n",
    "        \n",
    "        # Decoder Stage - 3\n",
    "        x_2d = F.max_unpool2d(x_30d, indices_2, kernel_size=2, stride=2, output_size=dim_2)\n",
    "        x_22d = F.relu(self.decoder_22(x_2d))\n",
    "        x_21d = F.relu(self.decoder_21(x_22d))\n",
    "        x_20d = F.relu(self.decoder_20(x_21d))\n",
    "\n",
    "        # Decoder Stage - 2\n",
    "        x_1d = F.max_unpool2d(x_20d, indices_1, kernel_size=2, stride=2, output_size=dim_1)\n",
    "        x_11d = F.relu(self.decoder_11(x_1d))\n",
    "        x_10d = F.relu(self.decoder_10(x_11d))\n",
    "\n",
    "        # Decoder Stage - 1\n",
    "        x_0d = F.max_unpool2d(x_10d, indices_0, kernel_size=2, stride=2, output_size=dim_0)\n",
    "        x_01d = F.relu(self.decoder_01(x_0d))\n",
    "        x_00d = self.decoder_00(x_01d)\n",
    "\n",
    "        x_softmax = F.softmax(x_00d, dim=1)\n",
    "\n",
    "        return x_softmax\n",
    "\n",
    "model = CNNSEG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the summary of the model including the shape of the tensor at each step and the number of trainable parameters. Due to the fact that the network is fully convolutional and the use of max-pooling indices in up-sampling allows the model to have relatively less trainable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 96, 96]             640\n",
      "       BatchNorm2d-2           [-1, 64, 96, 96]             128\n",
      "            Conv2d-3           [-1, 64, 96, 96]          36,928\n",
      "       BatchNorm2d-4           [-1, 64, 96, 96]             128\n",
      "            Conv2d-5          [-1, 128, 48, 48]          73,856\n",
      "       BatchNorm2d-6          [-1, 128, 48, 48]             256\n",
      "            Conv2d-7          [-1, 128, 48, 48]         147,584\n",
      "       BatchNorm2d-8          [-1, 128, 48, 48]             256\n",
      "            Conv2d-9          [-1, 256, 24, 24]         295,168\n",
      "      BatchNorm2d-10          [-1, 256, 24, 24]             512\n",
      "           Conv2d-11          [-1, 256, 24, 24]         590,080\n",
      "      BatchNorm2d-12          [-1, 256, 24, 24]             512\n",
      "           Conv2d-13          [-1, 256, 24, 24]         590,080\n",
      "      BatchNorm2d-14          [-1, 256, 24, 24]             512\n",
      "           Conv2d-15          [-1, 512, 12, 12]       1,180,160\n",
      "      BatchNorm2d-16          [-1, 512, 12, 12]           1,024\n",
      "           Conv2d-17          [-1, 512, 12, 12]       2,359,808\n",
      "      BatchNorm2d-18          [-1, 512, 12, 12]           1,024\n",
      "           Conv2d-19          [-1, 512, 12, 12]       2,359,808\n",
      "      BatchNorm2d-20          [-1, 512, 12, 12]           1,024\n",
      "           Conv2d-21            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-22            [-1, 512, 6, 6]           1,024\n",
      "           Conv2d-23            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-24            [-1, 512, 6, 6]           1,024\n",
      "           Conv2d-25            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-26            [-1, 512, 6, 6]           1,024\n",
      "  ConvTranspose2d-27            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-28            [-1, 512, 6, 6]           1,024\n",
      "  ConvTranspose2d-29            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-30            [-1, 512, 6, 6]           1,024\n",
      "  ConvTranspose2d-31            [-1, 512, 6, 6]       2,359,808\n",
      "      BatchNorm2d-32            [-1, 512, 6, 6]           1,024\n",
      "  ConvTranspose2d-33          [-1, 512, 12, 12]       2,359,808\n",
      "      BatchNorm2d-34          [-1, 512, 12, 12]           1,024\n",
      "  ConvTranspose2d-35          [-1, 512, 12, 12]       2,359,808\n",
      "      BatchNorm2d-36          [-1, 512, 12, 12]           1,024\n",
      "  ConvTranspose2d-37          [-1, 256, 12, 12]       1,179,904\n",
      "      BatchNorm2d-38          [-1, 256, 12, 12]             512\n",
      "  ConvTranspose2d-39          [-1, 256, 24, 24]         590,080\n",
      "      BatchNorm2d-40          [-1, 256, 24, 24]             512\n",
      "  ConvTranspose2d-41          [-1, 256, 24, 24]         590,080\n",
      "      BatchNorm2d-42          [-1, 256, 24, 24]             512\n",
      "  ConvTranspose2d-43          [-1, 128, 24, 24]         295,040\n",
      "      BatchNorm2d-44          [-1, 128, 24, 24]             256\n",
      "  ConvTranspose2d-45          [-1, 128, 48, 48]         147,584\n",
      "      BatchNorm2d-46          [-1, 128, 48, 48]             256\n",
      "  ConvTranspose2d-47           [-1, 64, 48, 48]          73,792\n",
      "      BatchNorm2d-48           [-1, 64, 48, 48]             128\n",
      "  ConvTranspose2d-49           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-50           [-1, 64, 96, 96]             128\n",
      "  ConvTranspose2d-51            [-1, 4, 96, 96]           2,308\n",
      "================================================================\n",
      "Total params: 29,444,164\n",
      "Trainable params: 29,444,164\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 63.28\n",
      "Params size (MB): 112.32\n",
      "Estimated Total Size (MB): 175.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (1, 96, 96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordance with the SegNet paper, the loss function was chosen to be categorical cross-entropy. This is the natural choice for classification problems and is perfect for our case as we are performing pixel-wise classification. Unlike in the paper, we opted to use the Adam optimiser as it is currently one of the most popular optimisers. We allow pytorch to use GPU acceleration if it is available. Lastly, we initialise the learning rate at 0.001 as well as training, validation loss and dice score lists to store the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the learning rate\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "#Initialise cross entropy loss and adam optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=LEARNING_RATE)\n",
    "\n",
    "#if gpu is available then it sends it to gpu else it uses cpu \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "#Initialise list to store loss over training\n",
    "train_loss = list()\n",
    "val_loss = list()\n",
    "dice_score = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define some helper functions:\n",
    "- `categorical_dice` which computes the categorical dice between two masks for a specific class. Small correction has been made to the code provided.\n",
    "- `forward` which preprocesses the image tensor by making it the right dimension and dividing each element component-wise by 255 to make the tensor lie inside the hypercube (granted of very high dimension). The gradients get set to zero in the optimiser and we push the mini-batch forwards through the network and return the softmax output of the model as a tensor of shape Bx4x96x96 where B is the batch size, 4 corresponds to the four categories and 96x96 is the image pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical dice score between mask1 and mask2 along label_class\n",
    "def categorical_dice(mask1, mask2, label_class=1):\n",
    "    mask1_pos = (mask1 == label_class).numpy()\n",
    "    mask2_pos = (mask2 == label_class).numpy()\n",
    "    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
    "    return dice\n",
    "\n",
    "#Packages proprocessing and forward pass\n",
    "def forward(img):\n",
    "    #scales image so that each pixel lies between 0 and 1\n",
    "    img_scale = img/255.\n",
    "    \n",
    "    #reshapes the tensor \n",
    "    img_scale = img_scale.unsqueeze(1)\n",
    "    \n",
    "    #resets optimiser \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #pushes processed image through network\n",
    "    seg_soft = model(img_scale)\n",
    "    \n",
    "    return seg_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `TrainDataset` class defined above as well as pytorch's `DataLoader` functionality, we create two data loaders. One for the training set and one for the validation set. Initial value of 10 for batch_size was chosen but later will be changed for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "#create path to data\n",
    "data_path = './data/train'\n",
    "val_path = './data/val'\n",
    "#initialise number of workers and batch size for data loaders\n",
    "num_workers = 8\n",
    "batch_size = 10\n",
    "\n",
    "#load data into data loaders and put the into a dict type\n",
    "train_set = TrainDataset(data_path)\n",
    "val_set = TrainDataset(val_path)\n",
    "\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "validating_data_loader = DataLoader(dataset=val_set, num_workers=num_workers, batch_size=80, shuffle=True)\n",
    "\n",
    "\n",
    "data_loader = {'train':training_data_loader,\n",
    "              'validate':validating_data_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the `train()` function that is passed the data loader dictionary, optimiser and loss function as arguements and returns the train and validation loss as well as the dice score computed throughout the training. We train the model for 100 epochs and store the weights of the model that gave the lowest validation loss.\n",
    "\n",
    "Training is done in the standard way: we unpack the data from the data loader, it is then sent to the GPU (if it is available), pushed through the `forward()` function defined above that deals with preprocessing and doing the forward pass. The function returns the softmax activation of the model which is used to then compute the loss. The loss is passed backwards through the network and the optimiser takes a step. For each training step, the batch loss is computed and stored to keep track of training. \n",
    "\n",
    "For the validation set, the model is set to not train. In a similar fashion, the image is passed through but once it reaches the loss stage, the loss is computed and stored without pushing it back through the network and optimising. The validation set is only used to prevent the model from over-fitting on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function  CrossEntropyLoss()\n",
      "optimizer  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Mini Batch size set to  25\n",
      "cpu\n",
      "----------\n",
      "Epoch 1/100\n",
      "Train loss:  1.1907135546207428\n",
      "Found better\n",
      "Validation loss:  1.3220692873001099\n",
      "Dice score:  0.22676952450977486\n",
      "----------\n",
      "Epoch 2/100\n",
      "Train loss:  0.9651130139827728\n",
      "Validation loss:  1.5677934885025024\n",
      "Dice score:  0.11657515955943923\n",
      "----------\n",
      "Epoch 3/100\n",
      "Train loss:  0.9022946059703827\n",
      "Validation loss:  1.3890368938446045\n",
      "Dice score:  0.1887524770552379\n",
      "----------\n",
      "Epoch 4/100\n",
      "Train loss:  0.8685487508773804\n",
      "Validation loss:  1.459784746170044\n",
      "Dice score:  0.1515035741083483\n",
      "----------\n",
      "Epoch 5/100\n",
      "Train loss:  0.8509644567966461\n",
      "Found better\n",
      "Validation loss:  1.2911052703857422\n",
      "Dice score:  0.21818390794501083\n",
      "----------\n",
      "Epoch 6/100\n",
      "Train loss:  0.8388635963201523\n",
      "Validation loss:  1.3907243013381958\n",
      "Dice score:  0.17630057915548336\n",
      "----------\n",
      "Epoch 7/100\n",
      "Train loss:  0.8279061615467072\n",
      "Found better\n",
      "Validation loss:  1.1506547927856445\n",
      "Dice score:  0.3010661757290496\n",
      "----------\n",
      "Epoch 8/100\n",
      "Train loss:  0.8131911307573318\n",
      "Found better\n",
      "Validation loss:  1.1383442878723145\n",
      "Dice score:  0.3206100632437849\n",
      "----------\n",
      "Epoch 9/100\n",
      "Train loss:  0.8075046688318253\n",
      "Found better\n",
      "Validation loss:  1.0103939771652222\n",
      "Dice score:  0.39581769184947957\n",
      "----------\n",
      "Epoch 10/100\n",
      "Train loss:  0.7994929552078247\n",
      "Found better\n",
      "Validation loss:  0.9346132874488831\n",
      "Dice score:  0.5702276270023909\n",
      "----------\n",
      "Epoch 11/100\n",
      "Train loss:  0.7935550659894943\n",
      "Found better\n",
      "Validation loss:  0.8820152282714844\n",
      "Dice score:  0.6256347755791163\n",
      "----------\n",
      "Epoch 12/100\n",
      "Train loss:  0.7866571694612503\n",
      "Validation loss:  0.9199907779693604\n",
      "Dice score:  0.5799980976149852\n",
      "----------\n",
      "Epoch 13/100\n",
      "Train loss:  0.7835966497659683\n",
      "Found better\n",
      "Validation loss:  0.8260939717292786\n",
      "Dice score:  0.7793400333437324\n",
      "----------\n",
      "Epoch 14/100\n",
      "Train loss:  0.7812973111867905\n",
      "Found better\n",
      "Validation loss:  0.8244369626045227\n",
      "Dice score:  0.7726254816631224\n",
      "----------\n",
      "Epoch 15/100\n",
      "Train loss:  0.781312495470047\n",
      "Found better\n",
      "Validation loss:  0.8042582869529724\n",
      "Dice score:  0.7980406782494059\n",
      "----------\n",
      "Epoch 16/100\n",
      "Train loss:  0.7821792215108871\n",
      "Validation loss:  0.8124666213989258\n",
      "Dice score:  0.8078570285783678\n",
      "----------\n",
      "Epoch 17/100\n",
      "Train loss:  0.7777176797389984\n",
      "Validation loss:  0.8057911396026611\n",
      "Dice score:  0.7948453526746653\n",
      "----------\n",
      "Epoch 18/100\n",
      "Train loss:  0.7767532467842102\n",
      "Found better\n",
      "Validation loss:  0.7858651876449585\n",
      "Dice score:  0.8665895875047054\n",
      "----------\n",
      "Epoch 19/100\n",
      "Train loss:  0.7765694856643677\n",
      "Validation loss:  0.7874875664710999\n",
      "Dice score:  0.8606239159100522\n",
      "----------\n",
      "Epoch 20/100\n",
      "Train loss:  0.7742973864078522\n",
      "Validation loss:  0.7999722957611084\n",
      "Dice score:  0.8361182621329157\n",
      "----------\n",
      "Epoch 21/100\n",
      "Train loss:  0.7728110998868942\n",
      "Validation loss:  0.7921347618103027\n",
      "Dice score:  0.839702429519638\n",
      "----------\n",
      "Epoch 22/100\n",
      "Train loss:  0.7734437435865402\n",
      "Validation loss:  0.791905403137207\n",
      "Dice score:  0.8451088544444437\n",
      "----------\n",
      "Epoch 23/100\n",
      "Train loss:  0.7715963274240494\n",
      "Validation loss:  0.7914522886276245\n",
      "Dice score:  0.8405335253500767\n",
      "----------\n",
      "Epoch 24/100\n",
      "Train loss:  0.7699364274740219\n",
      "Validation loss:  0.7942946553230286\n",
      "Dice score:  0.8298489358318696\n",
      "----------\n",
      "Epoch 25/100\n",
      "Train loss:  0.7711904048919678\n",
      "Validation loss:  0.7886360883712769\n",
      "Dice score:  0.8526417470704489\n",
      "----------\n",
      "Epoch 26/100\n",
      "Train loss:  0.7693922966718674\n",
      "Validation loss:  0.7990959882736206\n",
      "Dice score:  0.8108179811394765\n",
      "----------\n",
      "Epoch 27/100\n",
      "Train loss:  0.7692176103591919\n",
      "Validation loss:  0.8135516047477722\n",
      "Dice score:  0.7488201233451363\n",
      "----------\n",
      "Epoch 28/100\n",
      "Train loss:  0.7708025723695755\n",
      "Validation loss:  0.7954375147819519\n",
      "Dice score:  0.8204205020289963\n",
      "----------\n",
      "Epoch 29/100\n",
      "Train loss:  0.7705888748168945\n",
      "Validation loss:  0.7889721393585205\n",
      "Dice score:  0.8557662743799844\n",
      "----------\n",
      "Epoch 30/100\n",
      "Train loss:  0.7692386209964752\n",
      "Validation loss:  0.7881215214729309\n",
      "Dice score:  0.8544552599525579\n",
      "----------\n",
      "Epoch 31/100\n",
      "Train loss:  0.7679693847894669\n",
      "Validation loss:  0.7877556085586548\n",
      "Dice score:  0.8573993082328671\n",
      "----------\n",
      "Epoch 32/100\n",
      "Train loss:  0.7685051709413528\n",
      "Validation loss:  0.7909676432609558\n",
      "Dice score:  0.8392440552594856\n",
      "----------\n",
      "Epoch 33/100\n",
      "Train loss:  0.7696546316146851\n",
      "Validation loss:  0.7907058000564575\n",
      "Dice score:  0.843678918594911\n",
      "----------\n",
      "Epoch 34/100\n",
      "Train loss:  0.7688171714544296\n",
      "Validation loss:  0.7924465537071228\n",
      "Dice score:  0.8427276269190623\n",
      "----------\n",
      "Epoch 35/100\n",
      "Train loss:  0.7668686360120773\n",
      "Validation loss:  0.791295051574707\n",
      "Dice score:  0.8447446343709792\n",
      "----------\n",
      "Epoch 36/100\n",
      "Train loss:  0.7659535557031631\n",
      "Validation loss:  0.7971954345703125\n",
      "Dice score:  0.8364002622470932\n",
      "----------\n",
      "Epoch 37/100\n",
      "Train loss:  0.7657280564308167\n",
      "Validation loss:  0.7888749837875366\n",
      "Dice score:  0.8512731761715345\n",
      "----------\n",
      "Epoch 38/100\n",
      "Train loss:  0.7644565552473068\n",
      "Found better\n",
      "Validation loss:  0.7857086062431335\n",
      "Dice score:  0.8610682258412886\n",
      "----------\n",
      "Epoch 39/100\n",
      "Train loss:  0.7642862945795059\n",
      "Validation loss:  0.7878301739692688\n",
      "Dice score:  0.8524169588766208\n",
      "----------\n",
      "Epoch 40/100\n",
      "Train loss:  0.7644973695278168\n",
      "Validation loss:  0.7952200770378113\n",
      "Dice score:  0.8381297284765309\n",
      "----------\n",
      "Epoch 41/100\n",
      "Train loss:  0.7639790177345276\n",
      "Validation loss:  0.7902313470840454\n",
      "Dice score:  0.8439417133644309\n",
      "----------\n",
      "Epoch 42/100\n",
      "Train loss:  0.7637474834918976\n",
      "Validation loss:  0.8004248142242432\n",
      "Dice score:  0.8021024140243056\n",
      "----------\n",
      "Epoch 43/100\n",
      "Train loss:  0.7624761164188385\n",
      "Validation loss:  0.8036282062530518\n",
      "Dice score:  0.7905518542677131\n",
      "----------\n",
      "Epoch 44/100\n",
      "Train loss:  0.7625364661216736\n",
      "Validation loss:  0.7961008548736572\n",
      "Dice score:  0.8191397845777368\n",
      "----------\n",
      "Epoch 45/100\n",
      "Train loss:  0.7613568305969238\n",
      "Validation loss:  0.7882742881774902\n",
      "Dice score:  0.8496944345788014\n",
      "----------\n",
      "Epoch 46/100\n",
      "Train loss:  0.7608749568462372\n",
      "Validation loss:  0.7935584783554077\n",
      "Dice score:  0.8336150719927363\n",
      "----------\n",
      "Epoch 47/100\n",
      "Train loss:  0.7611171454191208\n",
      "Validation loss:  0.7959949970245361\n",
      "Dice score:  0.8211810512807609\n",
      "----------\n",
      "Epoch 48/100\n",
      "Train loss:  0.760162740945816\n",
      "Validation loss:  0.79146409034729\n",
      "Dice score:  0.8389995824556631\n",
      "----------\n",
      "Epoch 49/100\n",
      "Train loss:  0.7596173137426376\n",
      "Validation loss:  0.7862577438354492\n",
      "Dice score:  0.8565079322939582\n",
      "----------\n",
      "Epoch 50/100\n",
      "Train loss:  0.7589291632175446\n",
      "Validation loss:  0.789368748664856\n",
      "Dice score:  0.8475914098556611\n",
      "----------\n",
      "Epoch 51/100\n",
      "Train loss:  0.7596737593412399\n",
      "Validation loss:  0.7875902056694031\n",
      "Dice score:  0.8530653628979905\n",
      "----------\n",
      "Epoch 52/100\n",
      "Train loss:  0.7581824213266373\n",
      "Validation loss:  0.7862759232521057\n",
      "Dice score:  0.8583253494723301\n",
      "----------\n",
      "Epoch 53/100\n",
      "Train loss:  0.7575518041849136\n",
      "Validation loss:  0.7882193922996521\n",
      "Dice score:  0.8524532027189187\n",
      "----------\n",
      "Epoch 54/100\n",
      "Train loss:  0.7587708979845047\n",
      "Validation loss:  0.7958544492721558\n",
      "Dice score:  0.8238574830670986\n",
      "----------\n",
      "Epoch 55/100\n",
      "Train loss:  0.7576386034488678\n",
      "Validation loss:  0.7885428667068481\n",
      "Dice score:  0.8512998650646377\n",
      "----------\n",
      "Epoch 56/100\n",
      "Train loss:  0.7580866068601608\n",
      "Validation loss:  0.786741316318512\n",
      "Dice score:  0.8574523599799445\n",
      "----------\n",
      "Epoch 57/100\n",
      "Train loss:  0.7580862194299698\n",
      "Validation loss:  0.7879181504249573\n",
      "Dice score:  0.8500800009637423\n",
      "----------\n",
      "Epoch 58/100\n",
      "Train loss:  0.7568899542093277\n",
      "Validation loss:  0.7869234681129456\n",
      "Dice score:  0.8559602577339749\n",
      "----------\n",
      "Epoch 59/100\n",
      "Train loss:  0.7580666542053223\n",
      "Found better\n",
      "Validation loss:  0.7823147177696228\n",
      "Dice score:  0.8698907348670057\n",
      "----------\n",
      "Epoch 60/100\n",
      "Train loss:  0.7565119713544846\n",
      "Validation loss:  0.7844937443733215\n",
      "Dice score:  0.8647562023090477\n",
      "----------\n",
      "Epoch 61/100\n",
      "Train loss:  0.7555841207504272\n",
      "Validation loss:  0.786000669002533\n",
      "Dice score:  0.8580056537630206\n",
      "----------\n",
      "Epoch 62/100\n",
      "Train loss:  0.7551572769880295\n",
      "Validation loss:  0.7879496812820435\n",
      "Dice score:  0.8496814694697119\n",
      "----------\n",
      "Epoch 63/100\n",
      "Train loss:  0.7580345869064331\n",
      "Validation loss:  0.791266143321991\n",
      "Dice score:  0.8371821045365361\n",
      "----------\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.7589363306760788\n",
      "Validation loss:  0.7885631322860718\n",
      "Dice score:  0.8456051578552478\n",
      "----------\n",
      "Epoch 65/100\n",
      "Train loss:  0.7565690577030182\n",
      "Found better\n",
      "Validation loss:  0.7822650671005249\n",
      "Dice score:  0.8739751615125346\n",
      "----------\n",
      "Epoch 66/100\n",
      "Train loss:  0.7563050985336304\n",
      "Found better\n",
      "Validation loss:  0.7806515097618103\n",
      "Dice score:  0.8784279491683631\n",
      "----------\n",
      "Epoch 67/100\n",
      "Train loss:  0.7555929571390152\n",
      "Validation loss:  0.7880361080169678\n",
      "Dice score:  0.8537350199268952\n",
      "----------\n",
      "Epoch 68/100\n",
      "Train loss:  0.7570546418428421\n",
      "Validation loss:  0.7930583357810974\n",
      "Dice score:  0.8313653018091544\n",
      "----------\n",
      "Epoch 69/100\n",
      "Train loss:  0.7563739269971848\n",
      "Validation loss:  0.7976248860359192\n",
      "Dice score:  0.8108627879512242\n",
      "----------\n",
      "Epoch 70/100\n",
      "Train loss:  0.756492093205452\n",
      "Validation loss:  0.786649763584137\n",
      "Dice score:  0.8570345196798982\n",
      "----------\n",
      "Epoch 71/100\n",
      "Train loss:  0.7567161917686462\n",
      "Validation loss:  0.7881408929824829\n",
      "Dice score:  0.8501422191830112\n",
      "----------\n",
      "Epoch 72/100\n",
      "Train loss:  0.7560946047306061\n",
      "Validation loss:  0.7887512445449829\n",
      "Dice score:  0.8477806058190085\n",
      "----------\n",
      "Epoch 73/100\n",
      "Train loss:  0.7550032287836075\n",
      "Validation loss:  0.7910738587379456\n",
      "Dice score:  0.8413192689182609\n",
      "----------\n",
      "Epoch 74/100\n",
      "Train loss:  0.7545738071203232\n",
      "Validation loss:  0.7865356206893921\n",
      "Dice score:  0.8563558329762859\n",
      "----------\n",
      "Epoch 75/100\n",
      "Train loss:  0.754996120929718\n",
      "Validation loss:  0.7853888869285583\n",
      "Dice score:  0.8590751516448595\n",
      "----------\n",
      "Epoch 76/100\n",
      "Train loss:  0.75349460542202\n",
      "Validation loss:  0.788053572177887\n",
      "Dice score:  0.8513874110112525\n",
      "----------\n",
      "Epoch 77/100\n",
      "Train loss:  0.7541555464267731\n",
      "Validation loss:  0.7883115410804749\n",
      "Dice score:  0.8491616174428389\n",
      "----------\n",
      "Epoch 78/100\n",
      "Train loss:  0.752735823392868\n",
      "Validation loss:  0.7880625128746033\n",
      "Dice score:  0.853597488072604\n",
      "----------\n",
      "Epoch 79/100\n",
      "Train loss:  0.7527061253786087\n",
      "Validation loss:  0.7902823686599731\n",
      "Dice score:  0.8456962012445517\n",
      "----------\n",
      "Epoch 80/100\n",
      "Train loss:  0.7531734555959702\n",
      "Validation loss:  0.7962027788162231\n",
      "Dice score:  0.8221635801096785\n",
      "----------\n",
      "Epoch 81/100\n",
      "Train loss:  0.754548653960228\n",
      "Validation loss:  0.789320707321167\n",
      "Dice score:  0.8460217881710688\n",
      "----------\n",
      "Epoch 82/100\n",
      "Train loss:  0.7535200864076614\n",
      "Validation loss:  0.7881218194961548\n",
      "Dice score:  0.8514929808237959\n",
      "----------\n",
      "Epoch 83/100\n",
      "Train loss:  0.7529062032699585\n",
      "Validation loss:  0.7897557616233826\n",
      "Dice score:  0.8459727648788973\n",
      "----------\n",
      "Epoch 84/100\n",
      "Train loss:  0.7537729889154434\n",
      "Validation loss:  0.7911399006843567\n",
      "Dice score:  0.8434352452970414\n",
      "----------\n",
      "Epoch 85/100\n",
      "Train loss:  0.7533315122127533\n",
      "Validation loss:  0.789841890335083\n",
      "Dice score:  0.8440067504075552\n",
      "----------\n",
      "Epoch 86/100\n",
      "Train loss:  0.7518586814403534\n",
      "Validation loss:  0.7865283489227295\n",
      "Dice score:  0.8569868845911768\n",
      "----------\n",
      "Epoch 87/100\n",
      "Train loss:  0.7521084994077682\n",
      "Validation loss:  0.7895856499671936\n",
      "Dice score:  0.8541959254029126\n",
      "----------\n",
      "Epoch 88/100\n",
      "Train loss:  0.7514354735612869\n",
      "Validation loss:  0.7841120958328247\n",
      "Dice score:  0.8665574147475055\n",
      "----------\n",
      "Epoch 89/100\n",
      "Train loss:  0.7515877485275269\n",
      "Validation loss:  0.7885658144950867\n",
      "Dice score:  0.8496321359935434\n",
      "----------\n",
      "Epoch 90/100\n",
      "Train loss:  0.7514000087976456\n",
      "Validation loss:  0.7898387312889099\n",
      "Dice score:  0.8443805521878335\n",
      "----------\n",
      "Epoch 91/100\n",
      "Train loss:  0.7517229467630386\n",
      "Validation loss:  0.78584223985672\n",
      "Dice score:  0.8604004377053237\n",
      "----------\n",
      "Epoch 92/100\n",
      "Train loss:  0.7519412934780121\n",
      "Validation loss:  0.7844743728637695\n",
      "Dice score:  0.8687726835651901\n",
      "----------\n",
      "Epoch 93/100\n",
      "Train loss:  0.7530656605958939\n",
      "Validation loss:  0.7908313274383545\n",
      "Dice score:  0.8422297267923564\n",
      "----------\n",
      "Epoch 94/100\n",
      "Train loss:  0.7531257122755051\n",
      "Validation loss:  0.7888680100440979\n",
      "Dice score:  0.8488521395865941\n",
      "----------\n",
      "Epoch 95/100\n",
      "Train loss:  0.7530534565448761\n",
      "Validation loss:  0.7848602533340454\n",
      "Dice score:  0.8611417046695978\n",
      "----------\n",
      "Epoch 96/100\n",
      "Train loss:  0.7527370154857635\n",
      "Validation loss:  0.7936305403709412\n",
      "Dice score:  0.8418902760570531\n",
      "----------\n",
      "Epoch 97/100\n",
      "Train loss:  0.7525074481964111\n",
      "Validation loss:  0.7892054915428162\n",
      "Dice score:  0.860090548958376\n",
      "----------\n",
      "Epoch 98/100\n",
      "Train loss:  0.7529978156089783\n",
      "Validation loss:  0.7820858359336853\n",
      "Dice score:  0.8711614898150379\n",
      "----------\n",
      "Epoch 99/100\n",
      "Train loss:  0.7521338015794754\n",
      "Found better\n",
      "Validation loss:  0.7804208397865295\n",
      "Dice score:  0.877760516852967\n",
      "----------\n",
      "Epoch 100/100\n",
      "Train loss:  0.751831665635109\n",
      "Validation loss:  0.7834588885307312\n",
      "Dice score:  0.8679260240872879\n"
     ]
    }
   ],
   "source": [
    "def train(data_loader, optimizer, loss_fn):\n",
    "    print('Loss function ', loss_fn )\n",
    "    print('optimizer ', optimizer)\n",
    "    print('Mini Batch size set to ', batch_size)\n",
    "    print(device)\n",
    "\n",
    "\n",
    "    EPOCHS = 100\n",
    "    best_val = 100\n",
    "\n",
    "    for epochs in range(EPOCHS):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epochs+1, EPOCHS))\n",
    "\n",
    "        #TRAINING\n",
    "        model.train(True)\n",
    "\n",
    "        batch_loss = 0.0\n",
    "\n",
    "        for index_train, sample in enumerate(data_loader['train']):\n",
    "\n",
    "            #unpack data and send it to device (GPU/CPU)\n",
    "            img, mask = sample\n",
    "            img, mask = img.to(device), mask.to(device)\n",
    "\n",
    "            #preprocess and pass through network\n",
    "            soft_mask = forward(img)\n",
    "\n",
    "            #compute and push back loss\n",
    "            loss = loss_fn(soft_mask, mask.long())\n",
    "            loss.backward()\n",
    "\n",
    "            #optimise on the loss\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        train_loss.append(batch_loss / (index_train + 1))\n",
    "\n",
    "        print(\"Train loss: \", train_loss[-1])\n",
    "\n",
    "        #VALIDATING\n",
    "        model.train(False)\n",
    "\n",
    "        batch_loss = 0.0\n",
    "\n",
    "        for index_val, sample in enumerate(data_loader['validate']):\n",
    "            #unpack data and send to device\n",
    "            img, mask = sample\n",
    "            img, mask = img.to(device), mask.to(device)\n",
    "\n",
    "            #preprocess and pass through network\n",
    "            soft_mask = forward(img)\n",
    "\n",
    "            #compute loss\n",
    "            loss = loss_fn(soft_mask, mask.long())\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        val_loss.append(batch_loss / (index_val + 1))\n",
    "\n",
    "        #compute and store categorical dice score\n",
    "        dice_score_batch = np.mean([categorical_dice(mask, torch.argmax(soft_mask, dim=1), label_class=c) \n",
    "                                    for c in [0,1,2,3]])\n",
    "\n",
    "        dice_score.append(dice_score_batch)\n",
    "\n",
    "        #check if this weight results in lowest validation loss\n",
    "        if val_loss[-1] < best_val:\n",
    "            torch.save([model.state_dict(), val_loss[-1]], './segnet.pth')\n",
    "            best_val = val_loss[-1]\n",
    "            print(\"Found better\")\n",
    "\n",
    "        print(\"Validation loss: \", val_loss[-1])\n",
    "        print(\"Dice score: \", dice_score_batch)\n",
    "    return train_loss, val_loss, dice_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the train function trains the model and returns the lists outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss, dice_score = train(data_loader, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3gUlEQVR4nO3dd3hc1bno/+87XdKoWnKVK3HF3TLNwRhIbkwJLSbBIYAPARJ+JCSUEFJxCk+SG99zCDkBjkMx5EcwuZAQEuCQgykGHIpsHBtjG2xwkZtk2erS1HX/WKNiW83SHkszej/Po0eamT1rrz179O613732WmKMQSmlVOpz9XUFlFJKOUMDulJKpQkN6EoplSY0oCulVJrQgK6UUmlCA7pSSqUJDehKdUFEHhCRH/V1PZTqigZ01S+IyA4R+UwfrHeFiPz8qOfGiIgREQ+AMebrxpifdaOsPtkGpZppQFeqH2g+eCjVGxrQVb8mIn4RuUdE9iZ+7hERf+K1QhH5u4hUicghEXldRFyJ174rIntEpFZEtorIub2oQ0srvqN1isgfgFHA30SkTkTuSCx/kYhsSiz/qohMblPujkQ9NwD1IvIdEXn6qHXfKyK/6Wnd1cCirQLV3/0AOA2YCRjgr8APgR8BtwFlQFFi2dMAIyITgW8Ac40xe0VkDOB2qD7trtMYc5WInAlcZ4x5CUBEJgBPAJcArwK3YAP+FGNMOPH+xcAFwEEgD1gqInnGmKpEq/0K4DyH6q7SnLbQVX93JfBTY0y5MaYC+AlwVeK1CDAMGG2MiRhjXjd2cKIY4AemiIjXGLPDGLO9k3XcnmhBV4lIFbChk2U7Wmd7vgQ8Z4z5H2NMBFgGZABntFnmXmPMbmNMozFmH7AauDzx2kLgoDFmbSf1UaqFBnTV3w0HdrZ5vDPxHMCvgW3AP0TkYxG5E8AYsw34NrAUKBeRlSIynI4tM8bkNf8A0ztZtt11dqfuxpg4sBsY0WaZ3Ue951HgK4m/vwL8oZPylTqCBnTV3+0FRrd5PCrxHMaYWmPMbcaYccBFwK3NuXJjzB+NMZ9OvNcAv3KiMp2tM7GeDusuIgKMBPa0LfKo9zwDTBeRqcCFwONO1FsNDBrQVX/iFZFAmx8PNgf9QxEpEpFC4MfA/w8gIheKyKcSgbIam2qJi8hEETkncfG0CWgE4k5UsKN1Jl4+AIxrs/ifgAtE5FwR8WLz7yFgTUflG2OagKeAPwLvGGN2OVFvNTBoQFf9yfPY4Nv8sxT4OVCKzWtvBNYlngMYD7wE1AH/BO4zxryCzZ//EnuhcT8wGPieQ3XsaJ0Av8AefKpE5HZjzFZs2uS3ibp8Hvh8mwuiHXkUmIamW9RxEp3gQqn+RURGAVuAocaYmr6uj0od2kJXqh9J9KO/FVipwVwdL+2HrlQ/ISJZ2Dz8TmyXRaWOi6ZclFIqTXSZchGRh0WkXETe72SZBSKyPnGL82vOVlEppVR3dNlCF5H52Cv6jxljprbzeh62G9ZCY8wuERlsjCnvasWFhYVmzJgxPaq0UkoNVGvXrj1ojClq77Uuc+jGmNWJsTA68mXgz839ZbsTzAHGjBlDaWlpdxZVSimVICI7O3rNiV4uE4D8xEhya0Xk6k4qcoOIlIpIaUVFhQOrVkop1cyJgO4B5mBHjPsc8KPEKHPHMMYsN8aUGGNKioraPWNQSinVQ050WywDKo0x9dgxnVcDM4APHShbKaVUNzkR0P8K/Gdi3A0fcCrwHw6Uq5RyWCQSoaysjKampr6uiupCIBCguLgYr9fb7fd0GdBF5AlgAVAoImXAXYAXwBjzgDFms4j8N3asjTjwoDGmwy6OSqm+U1ZWRnZ2NmPGjMGOL6b6I2MMlZWVlJWVMXbs2G6/rzu9XBZ3Y5lfY8eJVkr1Y01NTRrMU4CIMGjQII6384iO5aLUAKPBPDX0ZD+ldkCPRWHdYxCP9XVNlFKqz6V2QN/xOjz7Tdj5Zl/XRCnVDZWVlcycOZOZM2cydOhQRowY0fI4HO58mPjS0lJuvvnmLtdxxhlndLlMd7z66qtceOGFjpR1oqT2aItN1fZ3Q2Xf1kMp1S2DBg1i/fr1ACxdupRgMMjtt9/e8no0GsXjaT8slZSUUFJS0uU61qzpcEKotJfaLfRwnf3dcKhv66GU6rElS5bw9a9/nVNPPZU77riDd955h9NPP51Zs2ZxxhlnsHXrVuDIFvPSpUu59tprWbBgAePGjePee+9tKS8YDLYsv2DBAhYtWsSkSZO48soraR676vnnn2fSpEnMmTOHm2++ucuW+KFDh7jkkkuYPn06p512Ghs2bADgtddeaznDmDVrFrW1tezbt4/58+czc+ZMpk6dyuuvv+74Z9aR1G6hh2rt78bDfVsPpVLQT/62iQ/2OjuHxpThOdz1+ZOP+31lZWWsWbMGt9tNTU0Nr7/+Oh6Ph5deeonvf//7PP3008e8Z8uWLbzyyivU1tYyceJEbrzxxmP6bL/33nts2rSJ4cOHM2/ePN58801KSkr42te+xurVqxk7diyLF3fZkY+77rqLWbNm8cwzz/Dyyy9z9dVXs379epYtW8bvfvc75s2bR11dHYFAgOXLl/O5z32OH/zgB8RiMRoaGo778+ipFA/oiRa6BnSlUtrll1+O2+0GoLq6mmuuuYaPPvoIESESibT7ngsuuAC/34/f72fw4MEcOHCA4uLiI5Y55ZRTWp6bOXMmO3bsIBgMMm7cuJb+3YsXL2b58uWd1u+NN95oOaicc845VFZWUlNTw7x587j11lu58sorueyyyyguLmbu3Llce+21RCIRLrnkEmbOnNmbj+a4pHZADze30Kv6tBpKpaKetKSTJSsrq+XvH/3oR5x99tn85S9/YceOHSxYsKDd9/j9/pa/3W430Wi0R8v0xp133skFF1zA888/z7x583jxxReZP38+q1ev5rnnnmPJkiXceuutXH11h2MWOiq1c+jaQlcq7VRXVzNixAgAVqxY4Xj5EydO5OOPP2bHjh0APPnkk12+58wzz+Txxx8HbG6+sLCQnJwctm/fzrRp0/jud7/L3Llz2bJlCzt37mTIkCFcf/31XHfddaxbt87xbehIagf0sAZ0pdLNHXfcwfe+9z1mzZrleIsaICMjg/vuu4+FCxcyZ84csrOzyc3N7fQ9S5cuZe3atUyfPp0777yTRx99FIB77rmHqVOnMn36dLxeL+eddx6vvvoqM2bMYNasWTz55JN861vfcnwbOtJnc4qWlJSYXk9w8cRi2Po8FE2Cm952pmJKpbHNmzczefLkvq5Gn6urqyMYDGKM4aabbmL8+PHccsstfV2tY7S3v0RkrTGm3f6bqd1C114uSqke+P3vf8/MmTM5+eSTqa6u5mtf+1pfV8kRKX5RtE3KxRjQMSqUUt1wyy239MsWeW+leAs9EdBjYYicuL6eSinVH6V4QK8FsX1XNe2ilBrougzoIvKwiJSLSLuTVojIAhGpFpH1iZ8fO1/NDoTrIGe4/Vtv/1dKDXDdaaGvABZ2sczrxpiZiZ+f9r5a3RCP24CeN8o+1ha6UmqA6zKgG2NWA/2v+Rupt79zR9rfGtCV6vfOPvtsXnzxxSOeu+eee7jxxhs7fM+CBQto7uJ8/vnnU1VVdcwyS5cuZdmyZZ2u+5lnnuGDDz5oefzjH/+Yl1566Thq377+NMyuUzn000XkXyLygoicmPuJmy+IagtdqZSxePFiVq5cecRzK1eu7NYAWWBHSczLy+vRuo8O6D/96U/5zGc+06Oy+isnAvo6YLQxZgbwW+CZjhYUkRtEpFRESo93rrxjNPdBz9MWulKpYtGiRTz33HMtk1ns2LGDvXv3cuaZZ3LjjTdSUlLCySefzF133dXu+8eMGcPBgwcBuPvuu5kwYQKf/vSnW4bYBdvHfO7cucyYMYMvfOELNDQ0sGbNGp599lm+853vMHPmTLZv386SJUt46qmnAFi1ahWzZs1i2rRpXHvttYRCoZb13XXXXcyePZtp06axZcuWTrevr4fZ7XU/dGNMTZu/nxeR+0Sk0BhzsJ1llwPLwd4p2qsVNw/MlTUYPAEN6EodrxfuhP0bnS1z6DQ475cdvlxQUMApp5zCCy+8wMUXX8zKlSv54he/iIhw9913U1BQQCwW49xzz2XDhg1Mnz693XLWrl3LypUrWb9+PdFolNmzZzNnzhwALrvsMq6//noAfvjDH/LQQw/xzW9+k4suuogLL7yQRYsWHVFWU1MTS5YsYdWqVUyYMIGrr76a+++/n29/+9sAFBYWsm7dOu677z6WLVvGgw8+2OH29fUwu71uoYvIUEnMZioipyTKTP4UQs0pF38QMvKhsf+l+ZVSx2qbdmmbbvnTn/7E7NmzmTVrFps2bToiPXK0119/nUsvvZTMzExycnK46KKLWl57//33OfPMM5k2bRqPP/44mzZt6rQ+W7duZezYsUyYMAGAa665htWrV7e8ftlllwEwZ86clgG9OvLGG29w1VVXAe0Ps3vvvfdSVVWFx+Nh7ty5PPLIIyxdupSNGzeSnZ3dadnd0WULXUSeABYAhSJSBtwFeAGMMQ8Ai4AbRSQKNAJXmBMxQEzzXaK+5oBelfRVKpVWOmlJJ9PFF1/MLbfcwrp162hoaGDOnDl88sknLFu2jHfffZf8/HyWLFlCU1NTj8pfsmQJzzzzDDNmzGDFihW8+uqrvapv8xC8vRl+90QNs9udXi6LjTHDjDFeY0yxMeYhY8wDiWCOMeY/jTEnG2NmGGNOM8acmAn9mnPo/uxEQNeUi1KpIBgMcvbZZ3Pttde2tM5ramrIysoiNzeXAwcO8MILL3Raxvz583nmmWdobGyktraWv/3tby2v1dbWMmzYMCKRSMuQtwDZ2dnU1tYeU9bEiRPZsWMH27ZtA+APf/gDZ511Vo+2ra+H2U3dsVyaA3pzC/3Qx31bH6VUty1evJhLL720JfXSPNzspEmTGDlyJPPmzev0/bNnz+ZLX/oSM2bMYPDgwcydO7fltZ/97GeceuqpFBUVceqpp7YE8SuuuILrr7+ee++9t+ViKEAgEOCRRx7h8ssvJxqNMnfuXL7+9a/3aLua5zqdPn06mZmZRwyz+8orr+ByuTj55JM577zzWLlyJb/+9a/xer0Eg0Eee+yxHq2zrdQdPveN/4CXlsL398ELd8C2l+C2zq9AKzXQ6fC5qWXgDJ8bqgNxgTfDttAbDtkRF5VSaoBK4YBeC75sO2RuRj7EQhBpbH29rBSiob6rn1JKnWCpG9DDdbbLItiADq0XRg/vgAfPhfef7pOqKdWf9VWaVR2fnuyn1A3ooVrbwwUgs8D+bg7o++zdWdTsPfH1UqofCwQCVFZWalDv54wxVFZWEggEjut9qdvLJVxne7jAsS30A5uOfKyUAqC4uJiysjJ6PfSGSrpAIEBxcfFxvSd1A3qok5TLgcTQ7Q3Jv2FVqVTi9XoZO3ZsX1dDJUlqp1yOaaEnbv9vbqHrpBdKqQEkdQN6uK41h962hR6qg8OfJB5rQFdKDRypG9DbXhT1ZoLbbwN6+Wb7nC9bW+hKqQElNQO6MUdeFG3ui954uDV/PvoMzaErpQaU1Azo0SaIR1svikKbgL7Jts6HzYCmaojH+q6eSil1AqVmL5fmsdB9bcYPzsiHhsM2zTLkZMgcBBg7rG7WoL6opVJKnVCp2UJvnq2obQs9s8BeBD3wfiKgN99spHl0pdTAkJoBvWW2orYt9Dyo3GbTLEOmtAZ0zaMrpQaILgO6iDwsIuUi8n4Xy80VkaiILOpsOUe0HQu9WUY+xOzEswyZChnNAV1b6EqpgaE7LfQVwMLOFhARN/Ar4B8O1Klr4fZa6Pmtfw+erCkXpdSA050p6FYDXUXFbwJPA+VOVKpLHbXQAfJGQSBXW+hKqQGn1zl0ERkBXArc341lbxCRUhEp7dXgQO220BMBfMjU1tdcXs2hK6UGDCcuit4DfNcYE+9qQWPMcmNMiTGmpKioqOdrbLko2k4LfcjJ9rdIa8+Xtt59qHWsF6WUSiNOBPQSYKWI7AAWAfeJyCUOlNux9lIuuYlhJkfMaX0uo+DIlEs0BM/dBuv+kNTqKaVUX+j1jUXGmJaxOEVkBfB3Y8wzvS23U+E6O36Ly9363KCT4KZ3oXB863OZg44cE726DDCt/diVUiqNdBnQReQJYAFQKCJlwF2AF8AY80BSa9eRtkPntlU04cjHmflw8KPWx4d3tL5fKaXSTJcB3RizuLuFGWOW9Ko23dV26NzOHJ1yqdplfzfn4JVSKo2k6J2itUdeEO1I80XR5vkTq3a2vl8ppdJMigb0uiMH5upI5iA7KmNzAD+cCOhhbaErpdJPagb0cDdb6BlHjefSknLRFrpSKv2kZkAPdTOHfvTt/5pyUUqlsdQM6G1nK+pMZmIc9IbDEK6H+gp792iotjWvrpRSaSI1A3p3L4pmtGmhV+22fxdNBBOzsx4ppVQaSb2AHovaYNyti6JtcujN6ZbmoQE07aKUSjOpNwVde7MVdSSQC4jtiy6Ju0rbBvTg4KRUUSml+kLqBfT2ZivqiMudmDz6EEQawBOAgpMS5WgLXSmVXlIwoLczMFdnMhN3i8Yjdqz05gOB9kVXSqWZ1Avo7Y2F3pmMAptDbzwMeaNb36ctdKVUmkm9i6LH3UIflOjlsivRQs85shyllEoTqRfQj7eFnlkAh3dBUxXkj269mKoBXSmVZlIvoBdOgAXfh5zh3Vs+Ix9C1fbvtjl0DehKqTSTejn0wZPtT3c190UHm0P3ZoK49KKoUirtdNlCF5GHRaRcRN7v4PWLRWSDiKxPTAD9aeer2QvNt/8D5I+xc436srWFrpRKO91JuawAFnby+ipghjFmJnAt8GDvq+Wg5tv/fcHWiaT9QZ3kQimVdroM6MaY1cChTl6vM6ZlpKssoH+NetWccskbbVvnYPPooZq+q5NSSiWBIxdFReRSEdkCPIdtpXe03A2JtExpRUWFE6vuWnPKJW9U63O+oObQlVJpx5GAboz5izFmEnAJ8LNOlltujCkxxpQUFRU5sequNadc8ke3PufXHLpSKv042m0xkZ4ZJyKFTpbbK5mDYNgMGDu/9TnNoSul0lCvuy2KyKeA7cYYIyKzAT9Q2euaOcXtga+tPvI5f4620JVSaafLgC4iTwALgEIRKQPuArwAxpgHgC8AV4tIBGgEvtTmImn/5M9uHYZXKaXSRJcB3RizuIvXfwX8yrEanQi+YOs0dM09X5RSKsWl3q3/TvBng4nbMdKVUipNDNCA3jxAl14YVUqljwEa0HUIXaVU+hmYAb15LHW9MKqUSiMDM6DrELpKqTQ0QAO65tCVUulngAZ0zaErpdLPAA3oiZSL5tCVUmlkYAZ0n84rqpRKPwMzoHszQNyaQ1dKpZWBGdBFEiMuagtdKZU+BmZAB3thVCe5UEqlkYEb0H1BnYZOKZVWBm5A11mLlFJpZgAHdJ21SCmVXroM6CLysIiUi8j7Hbx+pYhsEJGNIrJGRGY4X80k0Ba6UirNdKeFvgJY2MnrnwBnGWOmYSeIXu5AvZLPl60XRZVSaaU7MxatFpExnby+ps3Dt4BiB+qVfNpCV0qlGadz6F8FXujoRRG5QURKRaS0oqLC4VUfp+aA3s+nP1VKqe5yLKCLyNnYgP7djpYxxiw3xpQYY0qKiop6tJ7qhgjv7TpMUyTWw5om+IOAgXB978pRSql+wpGALiLTgQeBi40xlU6U2ZHXt1Vw6X1r2H2ol/OBtgzQpXl0pVR66HVAF5FRwJ+Bq4wxH/a+Sp0L+m3av6Yp2ruCfDrJhVIqvXR5UVREngAWAIUiUgbcBXgBjDEPAD8GBgH3iQhA1BhTkqwKZwe8ANSFehnQddYipVSa6U4vl8VdvH4dcJ1jNepCdsBWubYp0ruC/DqErlIqvaTcnaLNKZe63qZcNIeulEozKRfQm1vovU656CQXSqk0k3IBPcvnQcSBi6I6r6hSKs2kXEB3uYSgz+NcykUDulIqTaRcQAcIBjy9vyjq8YPLowFdKZU2UjOg+z29z6GLJG7/10kulFLpISUDenbAQ21vUy4ARZNh/RPwyerel6WUUn0sJQN6MOCltrctdIDLV0DeSHj8ctj2Uu/LU0qpPpSSAT074KGutzl0gOwhsOQ5GDQenlgM21b1vkyllOojqRnQ/Q6lXACyCuGaZyE4BN66z5kylVKqD6RkQHfkomhbmQUwdBrU7neuTKWUOsFSMqBnB7w0hGNEY3HnCg0O0YCulEppKRnQg4nb/+tDvZzkoq3sYdBwEKJh58pUSqkTKCUDenZigK7akAMXRlsKHWJ/15c7V6ZSSp1AqRnQW4bQdTCPHhxqf2vaRSmVoroM6CLysIiUi8j7Hbw+SUT+KSIhEbnd+SoeK+jUiIttZWtAV0qltu600FcACzt5/RBwM7DMiQp1R/OsRb0ez+WIQpsD+j7nylRKqROoy4BujFmNDdodvV5ujHkXcDC6dq55kgtHUy5ZRSAuqDvgXJlKKXUCpXQO3dGUi8sNWYM15aKUSlknNKCLyA0iUioipRUVFT0uJykXRcH2dNGArpRKUSc0oBtjlhtjSowxJUVFRT0uJ8Prxu2S3k9ycbTsYVCnAV0plZpSMuUiIgT9DkxycTS9W1QplcI8XS0gIk8AC4BCESkD7gK8AMaYB0RkKFAK5ABxEfk2MMUYk9SZI4J+jzND6LaVPQzqD0IsAm6vs2UrpVSSdRnQjTGLu3h9P1DsWI26yQ6hm4QcOgbqyiF3hLNlK6VUkqVkygUcnLXoiEKH2d+aR1dKpaCUDeiOD6ELNocOUKt90ZVSqSdlA3p2wOv8RVG9W1QplcJSNqAHA0looWcNBkTvFlVKpaSUDeiOTkPXzO2xQwBoC10plYJSN6AHPISiccJRB2ctApt20Ry6UioFpWxAbx6gy/G0S/ZQbaErpVJSygb0pAyhC7ani+bQlVIpKGUDejBpA3QNg/oKiDlcrlJKJVnKBvTsZIyJDvZuURO3QV0ppVJI6gb0RMrF+Ry63i2qlEpNKRvQW+cVdTqHrnOLKqVSU8oG9ORNcqEBXSmVmlI2oCdlXlGAYOJuUQ3oSqkUk7IB3e9x4XVLEu4W9ULmIM2hK6VSTsoGdBEhO+B1PocO9sKo3i2qlEoxXQZ0EXlYRMpF5P0OXhcRuVdEtonIBhGZ7Xw12xf0J2GSC7CTW1TtdL5cpZRKou600FcACzt5/TxgfOLnBuD+3lere5IyyQVA0UQ4+JGdik4ppVJElwHdGLMaONTJIhcDjxnrLSBPRIY5VcHOJGVeUYCiyRCPwKGPnS9bKaWSxIkc+ghgd5vHZYnnjiEiN4hIqYiUVlT0/k7MpLXQB0+yv8s3O1+2UkolyQm9KGqMWW6MKTHGlBQVFfW6vKRdFC2cCAhUbHW+bKWUShInAvoeYGSbx8WJ55IuaRdFfZmQPxoqtIWulEodTgT0Z4GrE71dTgOqjTEnZEDxYCLlYoxxvvCiyVC+xflylVIqSTxdLSAiTwALgEIRKQPuArwAxpgHgOeB84FtQAPwb8mq7NGyAx6icUMoGifgdTtb+OBJsO0l29PF7XW2bKWUSoIuA7oxZnEXrxvgJsdqdByah9CtaYo4H9CLJtmeLpXbWy+SKqVUP5ayd4pCmyF0k9IXPRHENY+ulEoRKR3QkzavKEDhBEA0j66UShkpHdAH5/gB2HO40fnCfZmQP0Zb6EqplJHSAX3CkGw8LmHjnurkrGCw9nRRSqWOlA7oAa+biUOzkxfQiybBoe0QDSenfKWUclBKB3SA6cW5bCirTk5f9MGTIR61QV0ppfq5lA/o00bkUd0YYfehJOTRi3RMF6VU6kiDgJ4LkJy0S+F4EBdUaB5dKdX/pXxAnzA0iM/tYsOeKucL92bYni7aQldKpYCUD+h+j5tJw7LZWJasni5T4EC7kzUppVS/kvIBHWzaZeOeJF0YLZ5rJ7qoK3e+bKWUclDaBPTapig7KxucL3z0PPt75xrny1ZKKQelR0AvthdGNyTjwuiwGeDNPDag7/sXvHGP8+tTSqkeSouAPmFINj6Pi41lVc4X7vHZtMuuowL6K7+Al+6CUK3z61RKqR5Ii4DudbuYMiwneXeMjp4H+9+Hxir7uKkatq+yfx/ekZx1KqXUcepWQBeRhSKyVUS2icid7bw+WkRWicgGEXlVRIqdr2rnpo3I5f09NcTjSbgwOvoMwMDut+3jrS9ALDEcgAZ0pVQ/0WVAFxE38DvgPGAKsFhEphy12DLgMWPMdOCnwC+crmhXphXnUheK8kllvfOFF5eAyws737SPNz0DmYX270OfOL8+pZTqge600E8BthljPjbGhIGVwMVHLTMFeDnx9yvtvJ50s0bmAbBm20HnC/dmwIjZsPOfremW6V+CjHw4rAFdKdU/dCegjwB2t3lclniurX8BlyX+vhTIFpFBRxckIjeISKmIlFZUVPSkvh361OAgk4Zm89TaMkfLbTH6DNi7Dt5/2qZbTr7U3kWqKRelVD/h1EXR24GzROQ94CxgDxA7eiFjzHJjTIkxpqSoqMihVVsiwqI5xfyrrJoPDySh58noeXbkxVd+ATnFNg2TP1ZTLkqpfqM7AX0PMLLN4+LEcy2MMXuNMZcZY2YBP0g8V+VUJbvrklkj8LiEp5PRSh95ih2oq74cTr4ERGwLvXo3xJIwBZ5SSh2n7gT0d4HxIjJWRHzAFcCzbRcQkUIRaS7re8DDzlazewqDfs6eNJg/v7eHaCzubOGBXBg6zf495RL7u2CsbbXXJCnNo5RSx6HLgG6MiQLfAF4ENgN/MsZsEpGfishFicUWAFtF5ENgCHB3kurbpUVziqmoDbH6I2dz9ABMvgiGTrfpFrApF9C0i1KqX/B0ZyFjzPPA80c99+M2fz8FPOVs1Xrm7ImDKcjy8dTaMs6ZNMTZwuffbn+a5Y+xv/XCqFKqH0iLO0Xb8nlcXDJzBC99UM7h+iTPBZozHNw+7bqolOoX0i6gg027hGNxHlid5LlAXW7IG6UpF6VUv5CWAX3K8BwWnzKS5as/5s1k3GjUVv5YTbkopfqFtAzoAD+6cArjCrO45cn1VNaFkreigkRAT8bkGkopdRzSNqBn+jz8dvFsqhoifPfpDcmZzQjshdFQDTQeTk75SinVTWkb0MGmXu48bxIvbS7nnpc+Sk5Q166LSql+Iq0DOsC/zRvDZbNH8JtVH/G/X9zqfFBv6bqoAV0p1be61Q89lYkIyxbNIMPr5v5Xt9MYjvHjC6fgcokzK9CArpTqJ9I+oAO4XMLPL5lKps/N71//hKqGML9aNB2/x937wn2ZEBwKh3b0viyllOqFARHQwbbUv3/+ZPIyffz6xa3srW5i+VVzyMv09b5wHUZXKdUPpH0OvS0R4aazP8VvrpjJ+l1VXHbfGnZVNvS+4IKxmnJRSvW5ARXQm108cwSPX38qhxrCXPnQWxzsbT/1/DFQswce/Cw8fR2s+U8dUlcpdcINyIAOMHdMASv+7RQqakNc/1gpTZFj5uPovhmLYfbV4PHDrrfhHz+Ad/7LucoqpVQ3DNiADjBzZB73fGkW63dXceuf1hOP97BLY/5ouOi3sOTv8O0NMP5/wct3Q9Xurt+rlFIOGdABHWDh1KF8/7zJPL9xP9984j227u/l9HUicP4ywMDzt7cOCVC5HV7+OZSV9rrOvfLBs/DIBVB7oG/roZRyXLcCuogsFJGtIrJNRO5s5/VRIvKKiLwnIhtE5Hznq5o81505lm9/Zjyrthzgc/es5qqH3uadTw71vMD80XD29+HD/7aTSr95L9w/D1b/Gh48F574Mhz4wLkN6K6da+Dpr8LON+DZb+j4M0qlGenqzkkRcQMfAp8FyrBT0i02xnzQZpnlwHvGmPtFZArwvDFmTGfllpSUmNLSPm6tHuVwfZg/vrOLFWt2UFEb4vI5xXz//MnkZ/Wga2MsCr8/G/ZvsI8nng+f+Ql88FdYcy+EauGLj8KUi53diI5UfAgPfRayimD6F+GVu+GC/wNzrzsx63dKNGTPcio2w6TPQ7bDk5go1c+JyFpjTEl7r3WnhX4KsM0Y87ExJgysBI6OQgbISfydC+ztaWX7Un6Wj5vO/hSrv3M2Ny44iT+/t4dz//01/vTubiLHO0ep2wMX/w6KT4FFj8AVf4SiCXDWd+Bb/4JhM+C526ChgzMBY+CtB2DLc71vSdfuh8e/AG4vfOUpmP8dOOlcePGHcPCj3pV9ouzfCI9dAr8cBSvOt5/db+fAP38HsUhf106pfqE7LfRFwEJjzHWJx1cBpxpjvtFmmWHAP4B8IAv4jDFmbTtl3QDcADBq1Kg5O3fudGo7kmLzvhq+9+eNrN9dxYi8DK47cyxfmjuSTJ8D92Pt3wj/dZbtIXPJ7458LR6H52+D0sRc2xMWwvm/tpNpdKZmH2DsTErN9m2AJxZD4yFY8hyMmN267P1n2DKvfAqCRb3fpqO34aN/wFv3gScAC38Bg07q+n2Hd0LZu3DSOZBZYJ/b+BT89RsQyIGpX4Axn4acEfDyz2DbS1A0yV63GHvmkWVFQ1D+gf0MDmyyqbCpX4Dsoc5sY1kpfPwqzPzykZ95fxWqgy1/hw1Pwr5/2TPG2Vf1TV2iIfjXE1C+2Z4lFo7vm3o4wRh77ewE6ayF7lRAvzVR1v8RkdOBh4CpxpgOm7X9MeXSHmMMr26t4P5Xt/POjkMMyvJx87njWXzKKHyeXl5TfmkpvPEfcPWzMO4s+1w8Dn//Fqx7DOZ9y6ZIXvkFYGDcAnB5bEvb7QdvADwZULcfdr8D1bsBgUkXwOnfgPpy+MvXISPfniEMn3nk+rc8B09eZbtbnnIDnHEzZA3q6IOwd8PufhsiDXbqPZcH6g7YkSYP77Bf6uAQyBwE21bZtEhOsU0vxcJwzg/gtP/PzvR0tHA9vHEPvPkbiIXs9k25CAJ58O7vYdTpcPmjR6ZYjIGtL8B/fxeqdtmD42d/Bg0H4d0H4V8rIVxnl/Vm2nqLC8bOh5lX2nSXx39kecaAq4v9Wn/Q7rv3/tBa9qdvhTO+Ad6Mzt/bU9Vl8MlqiDbBiDkweIr9HnTFGLvP1j4KHzxjP4O8UfZ7tWctnHkbnP3Djrc5VGcPihVboHAijDyl58ErHoeqHbD5b/DP++z3VhLfhTnXwFl3HptCi0XtPR41e+3vunLIyIPsYfYgWnCSPRtuKxq2n01H9YzH7D70ZYI/u+P6VpfZxkX+GBj0qWOXjTTBKz+Ht5dD8VyY9gWYcklrQyRJehvQTweWGmM+l3j8PQBjzC/aLLMJG/R3Jx5/DJxmjCnvqNxUCehtle44xLJ/bOWtjw8xelAm3zxnPPMnFDI4O9CzAiONtpVsDCz8pQ1KH78CW5+HM2+Hc35ov5RVu2HVT2xrJh61KYZY2L4/2gSBXPuFGnmqDWalD7eOz148F770eMe55oPb4LVfwcb/a4N09lD7DxPIsy1rjw8Q2Pte4oDRjoyC1kHK6ivsP13heDjjm7ZFXH8Q/n4LfPgCFIyzre8xn7bB/uCHNvC//2f7Dzt1kf3n3vw325JsqoaSr9rPx9PBtYxwA7y+zF58dnkg2mgPCFMvgwmfg6HT7TDHh7bDhj/Zcqt22gPPrK/Yeux8w140bqqx21Iwzrbos4fZH7fH9lQ6+JE98wjXwWk3wvQr4LVf2voGh9pgO2hc4kBWYz+LhkrwZdn1ZRXZA/PQqe1vS1ONDSSHEus6+BHsfgsOfXzkcp6APTCdeRuMOu3YcoyxF+RX/9oGY1/Q7ouZX7bfk3gUnrvVNhwmXwRDp9n9W7PX1iFcZ3/XlB1Zbt5omHa5LSNYBFmD7UF9z1r7HandZ7+f8aitg9tr90moBsq3QKTeljNuAXz6Fhh8Mqz+3/Y7G4/Zzyg4xJ6N1eyB6j1gOrlHxJ9jv0uj59l171xjzz4KxsIpX4OZi209tjxnP48Dm2x9TQxcXvsZTr4Qxp5lz/q8Afv/9sa/w7o/QLxNOi9vFIz/nF3enwN/vcke7CZ/Hiq22u+yywsl18KCO5MW2Hsb0D3Yi6LnAnuwF0W/bIzZ1GaZF4AnjTErRGQysAoYYTopPBUDOrS22H/5wha2HrBdHMcVZjFndD5ThucwaWgOJxVl4XHbFk8sbqhpilDVECEUjTFzZN6RKZuPX4PHLmp97MmA+bfZgN7TllC4wZ7O1u6z5Xi7ccAp32JbnPUV0FhlA2m00bZ24hHbIhw73/7jZOTb52IRyCq0B5QjP6Rj624MbPozrP8j7HqrteUMNviOmAPn/hhGn976fKTRBpjupGrA/lO9+RsonACzrur4bCMeh09ehXcfsi18E4PcUTD6DBukDn1if6p2Qfiobqy5I+2Zztk/gMGTW5//ZDW8/V826B/62J5lgP2sMgrsGUhDZWuAGDIVpi2yf5dvtoHh8C4IVR+5vuxh9nrL2LPsWZw30wbPslJ7EG44aF879et2u/NG2gD+wndh1z/tek79Gpx8GfiDR5ZtjD1DXPVTwNjgnDPcHtB9Qfsz6CRbRuEE21rd8CR88hq0d/KdVWQDfnMQF7Et7HjE1nvwFPuZFc+FIVOOfG/ldptaq91nD4JN1ZAzzJaXNwpyi23ADQ6GpiqbMqzebQP4J6/ZM8Tm71FxiX1+Tyn4su36o012H489M3GQHmr375a/H3mwzCiwZ5Rg01Ezr7TfwcqPoGwtbH/Z/l+APfBc/DsY/1n7We7fCKUP2YOkLxvm324bFHmj7JlbfaXdJzvXwJh59ky6B3oV0BMFnA/cA7iBh40xd4vIT4FSY8yziZ4tvweC2Aukdxhj/tFZmaka0JvF4oYNZVW8u+MQ73xyiHW7qjhUH+7yfZk+N/9ryhA+P2M444qCFGT6yK5YZ4fzzRttv7AnMB/XJ2IR24qqK4eiiYkg0EfjxNUesMG3o+sToVp7UTkasq0+X1bXZcbjNngHco5N6dSVw+ZnbTpoT+L7nzPCXgcYdJINXLnF9oxi0KdsGR0J19uW7Zu/sQdiAAQwkFloD5CzvtJ+iqutxsO2IdGdAz/YM65DH9ttqS+3B60Rc+zBrq++u7X77Vll220oK4W1j9iDybTL7YGkvYZG+ebE2cVee6DwZtgDZN7IY9cTboDtq+yZ05wl7bfCyzfDP34E2/6n9bnMQfY7AfbAM/87toNED/Q6oCdDqgf0oxljqKgNsXl/LTsr61s6prgEcjK85GR4wcA/PjjA8xv3Ud3YeirnEvC6XXhcgtsljMjPZPLQbCYNy2bMoCyG52UwIi8Dr8dFfShKXShKls/DkBw/ku7BP51V77EHiIy83pUTaYQ962yLs2qXDeBzr+t9uap39v3LnjUe3mHPJvLH2rPA4bOOPNAfJw3o/Uw4Gqd0xyH21zRxqD5MdWOEcDROLG4Ix+LsrGxgy/4aDtR0PmhYhtfNmMIsRhdkMmpQJiPzM8j0eSivDVFe20QkFmd0QRajB2UyPC8Dj1twixCJGfZWNVJ2uIHy2hAusQcSn8dFUdDP0NwAQ3MDDMkOkJPhOeKgYYyhPhzjYG2Ig3Uh3C4hN8NLboaX/EyfcxOHKKXa1VlAHzDjofcnPo+LMz5V2OVyh+vD7D7cwN6qRvZUNRGLx8nye8jyeahtivDJwQZ2VNbzUXktL28tJxxtzWsG/R7cLjniTKA97kQAjnUwjo3f46Iw6CduDI2RGA3h2BHraSvgdTG2MMi4oixG5GW0BPksvxuPy4XbJWT53QzJCTA4209uhrfdMwxjjJ55KNUDGtD7sfwsH/lZPqYX53W5bDxuqKgL0RCOMTjbT5bf7trqhgg7D9Wzv7qJWNwQjRtcIgzPC1Ccn0lh0IeIEE+cHVTUhthX3cT+mibKa5oorw1RURvC4xIyfW4CPjcFmT4Kg34GBX3EjaG60V70LTvcyMcVdWwsq2bV5gM0Rbq+Gas5zSRCS/2MsQeHTJ+HgMeFx+3C4xZ8bhdF2X4GZwcozPbhFqH5MJSf6aUo209h0E/Q7yHgdRPwujHGEIsbIjFDps9NfqaP7IBHzyRUWtKAniZcLmFIzrEXtXIzvUzPzGN6cdfvD7jcjCzIZGRBpiN1aorEONwQpjEcIxo3RGOG2qYI5bUhDtQ0Ud0YIRY3xIwN4h6X4EkE2qZonIZwlKZInGgsTiRuCEXiHKwLsb38IAfrwsSNPTjFjT0QdJfbJWT53GT6PGT43GT53QT9HoJ+LyJQ2xShtimKCAzPzWB4XgZDcgIE/fY9bpdwsM4e6GqaouRneikM+snL9BKJxWmKxAlFW7vaCUJ+lo+ibD+DsnzUNkU5WBeisi5EToaX4vxMRuRnUBj0dTgt4t6qRtbuPMyuQw1UNYQ53BDB53ExeVgOU4ZlM3pQFj6PC6/Lhc/jajnzAnugLK9torIuTKbPTTDgaelpFTeGeOJAGo3ZfRHwuMjye/B7XHqmlGI0oKukCXjdDMtN0o02bRhjqAtFOVgXpqI2RH04SlM4RlM0hiB43PZA0RCOcbghwuH6MHWhKA3hKA1hm0aqa4qyp6oRYwzZAQ9DcwLEjGFHZT1vbjtIffjYvtA+j4ucgIfDDZEOU1bHq/ksItPnTpxluCg73Mi+6qaWZTK8bvIyvTSEY/zx7V0dltOcdquoDR3XAa+ZxyXkZfooDPoYFPSR5bNnPn6Pi7iBpmiMUCROlt/dcuE+4HVT1xShPhzD4xKG5gYYlptBQZYXsGdi8bihKRKnMRKjLhRhf3WI/dWNVDVGyM3wUpDloyDLR9BvDzwBr4u6UJSqhgh1oShul9jPxuNicE6A4bkBCoP+lrOu3qbsYnHDofowlfUhahqjxBMNDrCdZATbAPK6XfjcLnweaUkpetxCKLFtoWicTJ+bvESniBNxgNSArlKeiJAd8JId8DK2sBvdCo+TMTYANR8EonFDYdBPTsDTkq6qboxQ1WhbzQGPbSW7Ev+8MWOoqo9QUdfEwbow2X4Phdl+CrJ81DTaVNWeqkYq60L2gNMQpiEUIxSN0RSJUzKmgDmj8pgzuoDxQ4IEvO6Weu2rbuKDvTXsqWokEosTjRuaIrGW3lDhqGForp9hufYMoCkSpzYUpTEcRRIB1iWC1y24XS7cLmiKxKkPR6lrinK4IUJlXShxRhEmFI3TFInhEsHvdeH3uKkLRXhuw74eHTSauQSyA15qmyL0pBivW3CJtKTtfG6XPfMKeHCLEDOGeNwGZHcizYeBcCxOOBpv+eyiMUMoGutRHbrD53bhdQtfPXMct352guPla0BXqgsiQobPTYbPDRzb3czlkpbrHR3JCXgZNejYVFZh0M+4omA77+hevYbn2ZRQX4vFbbfdcDROMOAhy+8mEjPsq2pkb7VNrzX3qHOJkJG4xpHldzM0N0BR0I/H7SKWODgeqg/TELYHJXsW4CEv00t2wJMIunEawzHKa5vYm1hHPG5sTy6Xi3A03nJQi8UNbpcN+AZ7TaX5jMrnsa1sexZng23A66YwaK/H5GZ4cQkgNnVmsK31uDFEYnHCUXvtKRaPE4nZcgNeFwGPG7/XRX0oRnVjhOrGCKHEgSMSjTN9RG4nn2bPaUBXSvWaO5FeacvvgfFDshk/pJPxUtoppznl0j3JCYypasDPWKSUUulCA7pSSqUJDehKKZUmNKArpVSa0ICulFJpQgO6UkqlCQ3oSimVJjSgK6VUmuiz8dBFpALY2cO3FwIHHaxOqhiI2z0QtxkG5nYPxG2G49/u0caYovZe6LOA3hsiUtrRAO/pbCBu90DcZhiY2z0Qtxmc3W5NuSilVJrQgK6UUmkiVQP68r6uQB8ZiNs9ELcZBuZ2D8RtBge3OyVz6EoppY6Vqi10pZRSR9GArpRSaSLlArqILBSRrSKyTUTu7Ov6JIOIjBSRV0TkAxHZJCLfSjxfICL/IyIfJX7n93Vdk0FE3CLynoj8PfF4rIi8ndjnT4pId2c/SAkikiciT4nIFhHZLCKnD4R9LSK3JL7f74vIEyISSMd9LSIPi0i5iLzf5rl2969Y9ya2f4OIzD6edaVUQBcRN/A74DxgCrBYRKb0ba2SIgrcZoyZApwG3JTYzjuBVcaY8cCqxON09C1gc5vHvwL+wxjzKeAw8NU+qVXy/Ab4b2PMJGAGdtvTel+LyAjgZqDEGDMVcANXkJ77egWw8KjnOtq/5wHjEz83APcfz4pSKqADpwDbjDEfG2PCwErg4j6uk+OMMfuMMesSf9di/8FHYLf10cRijwKX9EkFk0hEioELgAcTjwU4B3gqsUhabbeI5ALzgYcAjDFhY0wVA2BfY6fAzBARD5AJ7CMN97UxZjVw6KinO9q/FwOPGestIE9EhnV3XakW0EcAu9s8Lks8l7ZEZAwwC3gbGGKM2Zd4aT8wpK/qlUT3AHcA8cTjQUCVMSaaeJxu+3wsUAE8kkgzPSgiWaT5vjbG7AGWAbuwgbwaWEt67+u2Otq/vYpxqRbQBxQRCQJPA982xtS0fc3Y/qZp1edURC4Eyo0xa/u6LieQB5gN3G+MmQXUc1R6JU33dT62NToWGA5kcWxaYkBwcv+mWkDfA4xs87g48VzaEREvNpg/boz5c+LpA82nX4nf5X1VvySZB1wkIjuw6bRzsPnlvMRpOaTfPi8DyowxbyceP4UN8Om+rz8DfGKMqTDGRIA/Y/d/Ou/rtjrav72KcakW0N8FxieuhPuwF1Ge7eM6OS6RN34I2GyM+fc2Lz0LXJP4+xrgrye6bslkjPmeMabYGDMGu29fNsZcCbwCLEosllbbbYzZD+wWkYmJp84FPiDN9zU21XKaiGQmvu/N2522+/ooHe3fZ4GrE71dTgOq26RmumaMSakf4HzgQ2A78IO+rk+StvHT2FOwDcD6xM/52HzyKuAj4CWgoK/rmsTPYAHw98Tf44B3gG3A/wX8fV0/h7d1JlCa2N/PAPkDYV8DPwG2AO8DfwD86bivgSew1wki2DOyr3a0fwHB9uTbDmzE9gLq9rr01n+llEoTqZZyUUop1QEN6EoplSY0oCulVJrQgK6UUmlCA7pSSqUJDehKKZUmNKArpVSa+H9q8C6ss18GPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label=\"Training loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss History\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# References\n",
    "Badrinarayanan, V., Kendall, A. and Cipolla, R., 2015. Segnet: A Deep Convolutional Encoder-Decoder Architecture For Image Segmentation. [online] arXiv.org. Available at: <https://arxiv.org/abs/1511.00561v3> [Accessed 14 December 2020].\n",
    "\n",
    "O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, et al.\n",
    "\"Deep Learning Techniques for Automatic MRI Cardiac Multi-structures Segmentation and\n",
    "Diagnosis: Is the Problem Solved ?\" in IEEE Transactions on Medical Imaging,\n",
    "vol. 37, no. 11, pp. 2514-2525, Nov. 2018\n",
    "doi: 10.1109/TMI.2018.2837502\n",
    "\n",
    "Chen, Chen et al. “Deep Learning for Cardiac Image Segmentation: A Review.” Frontiers in cardiovascular medicine vol. 7 25. 5 Mar. 2020, doi:10.3389/fcvm.2020.00025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
